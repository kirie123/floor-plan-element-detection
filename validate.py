import torch
import pandas as pd
from PIL import Image
import torchvision.transforms as T
import numpy as np
from pathlib import Path

from loss import detection_loss
from models.default_model import DummyModel


def decode_outputs(outputs, confidence_threshold=0.3, nms_threshold=0.5, stride=16, img_size=1024):
    """
    å°†æ¨¡å‹è¾“å‡ºè§£ç ä¸ºè¾¹ç•Œæ¡†ï¼ˆå¤„ç†å½’ä¸€åŒ–åæ ‡ï¼‰
    Args:
        outputs: æ¨¡å‹è¾“å‡ºå­—å…¸ {'heatmap', 'wh', 'offset'}
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        nms_threshold: éæå¤§å€¼æŠ‘åˆ¶é˜ˆå€¼
        stride: ä¸‹é‡‡æ ·å€æ•°
        img_size: å›¾åƒå°ºå¯¸ï¼ˆç”¨äºåå½’ä¸€åŒ–ï¼‰
    Returns:
        list: æ¯ä¸ªå…ƒç´ çš„æ ¼å¼ä¸º [xmin, ymin, xmax, ymax, confidence, class_id]
    """
    heatmap = torch.sigmoid(outputs['heatmap'])
    wh = outputs['wh']
    offset = outputs['offset']
    # åœ¨ decode_outputs å‡½æ•°å¼€å¤´æ·»åŠ 
    #print(f"Max heatmap value: {heatmap.max().item():.4f}")
    #print(f"Mean heatmap value: {heatmap.mean().item():.4f}")
    batch_size, num_classes, H, W = heatmap.shape
    detections = []

    for b in range(batch_size):
        batch_detections = []

        for cls_id in range(num_classes):
            # è·å–å½“å‰ç±»åˆ«çš„çƒ­åŠ›å›¾
            cls_heatmap = heatmap[b, cls_id]

            # æ‰¾åˆ°å³°å€¼ç‚¹ï¼ˆä½¿ç”¨æœ€å¤§æ± åŒ–å®ç°ç®€å•çš„NMSï¼‰
            pooled = torch.nn.functional.max_pool2d(
                cls_heatmap.unsqueeze(0), kernel_size=3, stride=1, padding=1
            )[0]

            # æ‰¾åˆ°å±€éƒ¨æœ€å¤§å€¼
            peak_mask = (cls_heatmap == pooled) & (cls_heatmap > confidence_threshold)
            peak_indices = torch.nonzero(peak_mask)

            for idx in peak_indices:
                y, x = idx[0].item(), idx[1].item()
                confidence = cls_heatmap[y, x].item()

                # è§£ç è¾¹ç•Œæ¡† - æ³¨æ„ï¼šç°åœ¨whæ˜¯åœ¨ç‰¹å¾å›¾å°ºåº¦ä¸Šçš„
                w_feat = torch.exp(wh[b, 0, y, x]).item()
                h_feat = torch.exp(wh[b, 1, y, x]).item()
                dx = offset[b, 0, y, x].item()
                dy = offset[b, 1, y, x].item()
                # è®¡ç®—ä¸­å¿ƒç‚¹åæ ‡ï¼ˆç‰¹å¾å›¾å°ºåº¦ï¼‰
                center_x_feat = x + dx
                center_y_feat = y + dy
                # è½¬æ¢åˆ°åŸå›¾å°ºåº¦
                center_x = center_x_feat * stride
                center_y = center_y_feat * stride
                w = w_feat * stride
                h = h_feat * stride

                # è®¡ç®—è¾¹ç•Œæ¡†åæ ‡ï¼ˆåƒç´ åæ ‡ï¼‰
                xmin = max(0, center_x - w / 2)
                ymin = max(0, center_y - h / 2)
                xmax = min(img_size, center_x + w / 2)
                ymax = min(img_size, center_y + h / 2)

                batch_detections.append([xmin, ymin, xmax, ymax, confidence, cls_id])

        # åº”ç”¨NMS
        if batch_detections:
            batch_detections = apply_nms(batch_detections, nms_threshold)
        detections.append(batch_detections)

    return detections


def apply_nms(detections, iou_threshold=0.5):
    """
    åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶
    """
    if not detections:
        return []

    # æŒ‰ç½®ä¿¡åº¦æ’åº
    detections = sorted(detections, key=lambda x: x[4], reverse=True)
    keep = []

    while detections:
        current = detections.pop(0)
        keep.append(current)

        detections = [det for det in detections if
                      iou(current[:4], det[:4]) < iou_threshold or
                      current[5] != det[5]]  # ä¸åŒç±»åˆ«ä¸æŠ‘åˆ¶

    return keep


def iou(box1, box2):
    """
    è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU
    """
    x1_1, y1_1, x2_1, y2_1 = box1
    x1_2, y1_2, x2_2, y2_2 = box2

    # è®¡ç®—äº¤é›†åŒºåŸŸ
    xi1 = max(x1_1, x1_2)
    yi1 = max(y1_1, y1_2)
    xi2 = min(x2_1, x2_2)
    yi2 = min(y2_1, y2_2)

    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)

    # è®¡ç®—å¹¶é›†åŒºåŸŸ
    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)
    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)
    union_area = box1_area + box2_area - inter_area

    return inter_area / union_area if union_area > 0 else 0


def predict_image(model, image_path, class_names, device, confidence_threshold=0.3, img_size=1024):
    """
    å¯¹å•å¼ å›¾åƒè¿›è¡Œé¢„æµ‹å¹¶ç”ŸæˆCSVæ ¼å¼ç»“æœ
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        image_path: å›¾åƒè·¯å¾„
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        device: è®¾å¤‡
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        img_size: å›¾åƒå°ºå¯¸
    Returns:
        pd.DataFrame: åŒ…å«é¢„æµ‹ç»“æœçš„DataFrameï¼Œæ ¼å¼ä¸è®­ç»ƒCSVç›¸åŒ
    """
    # å›¾åƒé¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    image = Image.open(image_path).convert('RGB')
    original_image = transform(image).unsqueeze(0).to(device)

    # æ¨ç†
    model.eval()
    with torch.no_grad():
        outputs = model(original_image)

    # è§£ç è¾“å‡º
    detections = decode_outputs(outputs, confidence_threshold=confidence_threshold, img_size=img_size, stride = 16)
    batch_detections = detections[0]  # å–ç¬¬ä¸€ä¸ªbatch

    # è½¬æ¢ä¸ºDataFrame
    results = []
    for detection in batch_detections:
        xmin, ymin, xmax, ymax, confidence, class_id = detection
        class_name = class_names[class_id]

        results.append({
            'filename': Path(image_path).name,
            'xmin': int(xmin),
            'ymin': int(ymin),
            'xmax': int(xmax),
            'ymax': int(ymax),
            'class': class_name,
            'confidence': confidence
        })

    return pd.DataFrame(results)


def predict_and_save_csv(model, image_path, output_csv_path, class_names, device, confidence_threshold=0.3, img_size=1024):
    """
    é¢„æµ‹å›¾åƒå¹¶ä¿å­˜ç»“æœä¸ºCSV
    """
    df = predict_image(model, image_path, class_names, device, confidence_threshold, img_size)
    df.to_csv(output_csv_path, index=False)
    print(f"âœ… é¢„æµ‹ç»“æœä¿å­˜è‡³: {output_csv_path}")
    return df


# æ‰¹é‡é¢„æµ‹å‡½æ•°
def predict_batch(model, image_dir, output_dir, class_names, device, confidence_threshold=0.3, img_size=1024):
    """
    æ‰¹é‡é¢„æµ‹ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒ
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)

    image_paths = list(Path(image_dir).glob("*.jpg")) + list(Path(image_dir).glob("*.png"))

    for image_path in image_paths:
        output_csv_path = output_dir / f"{image_path.stem}_pred.csv"
        predict_and_save_csv(model, image_path, output_csv_path, class_names, device, confidence_threshold, img_size)

    print(f"ğŸ‰ æ‰¹é‡é¢„æµ‹å®Œæˆ! å…±å¤„ç† {len(image_paths)} å¼ å›¾åƒ")




def validate_model(model, val_loader, device, num_classes, img_size=1024, stride = 16):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
    """
    model.eval()
    total_val_loss = 0
    hm_val_loss = 0
    wh_val_loss = 0
    off_val_loss = 0

    with torch.no_grad():
        for batch_idx, (images, targets) in enumerate(val_loader):
            images = images.to(device)

            outputs = model(images)
            loss, loss_dict = detection_loss(outputs, targets, num_classes=num_classes,stride = stride, img_size=img_size)

            total_val_loss += loss_dict["total"].item()
            hm_val_loss += loss_dict["hm"].item()
            wh_val_loss += loss_dict["wh"].item()
            off_val_loss += loss_dict["off"].item()

    num_batches = len(val_loader)
    avg_loss = total_val_loss / num_batches
    avg_hm = hm_val_loss / num_batches
    avg_wh = wh_val_loss / num_batches
    avg_off = off_val_loss / num_batches

    return {
        "total": avg_loss,
        "hm": avg_hm,
        "wh": avg_wh,
        "off": avg_off
    }


def calculate_metrics(model, val_loader, device, class_names,stride = 16, iou_threshold=0.5, img_size=1024):
    """
    è®¡ç®—æ›´è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆmAPç­‰ï¼‰
    """
    model.eval()
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for images, targets in val_loader:
            images = images.to(device)
            outputs = model(images)

            # è§£ç é¢„æµ‹
            predictions = decode_outputs(outputs, confidence_threshold=0.1, img_size=img_size, stride = stride)

            # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
            for i, (preds, target) in enumerate(zip(predictions, targets)):
                image_id = target['image_id'].item()

                # å¤„ç†é¢„æµ‹
                for pred in preds:
                    xmin, ymin, xmax, ymax, confidence, class_id = pred
                    all_predictions.append({
                        'image_id': image_id,
                        'bbox': [xmin, ymin, xmax, ymax],
                        'score': confidence,
                        'category_id': class_id
                    })

                # å¤„ç†çœŸå®æ ‡æ³¨ - éœ€è¦åå½’ä¸€åŒ–åˆ°åƒç´ åæ ‡
                boxes = target['boxes'].cpu().numpy() * img_size  # åå½’ä¸€åŒ–
                labels = target['labels'].cpu().numpy()

                for box, label in zip(boxes, labels):
                    all_targets.append({
                        'image_id': image_id,
                        'bbox': box.tolist(),
                        'category_id': label
                    })

    # è®¡ç®—APï¼ˆç®€åŒ–ç‰ˆï¼‰
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä½ å¯èƒ½æƒ³ä½¿ç”¨æ›´æˆç†Ÿçš„è¯„ä¼°åº“å¦‚torchmetricsæˆ–pycocotools
    aps = []
    for class_id in range(len(class_names)):
        class_predictions = [p for p in all_predictions if p['category_id'] == class_id]
        class_targets = [t for t in all_targets if t['category_id'] == class_id]

        if not class_targets:
            aps.append(0.0)
            continue

        # ç®€åŒ–çš„APè®¡ç®—ï¼ˆå®é™…åº”è¯¥è®¡ç®—PRæ›²çº¿ä¸‹é¢ç§¯ï¼‰
        matched = 0
        for pred in sorted(class_predictions, key=lambda x: x['score'], reverse=True):
            for target in class_targets:
                if target['image_id'] == pred['image_id'] and \
                        iou(pred['bbox'], target['bbox']) > iou_threshold:
                    matched += 1
                    break

        precision = matched / len(class_predictions) if class_predictions else 0
        recall = matched / len(class_targets) if class_targets else 0

        # ç®€åŒ–çš„APï¼ˆå®é™…åº”è¯¥æ›´å¤æ‚ï¼‰
        ap = 2 * precision * recall / (precision + recall + 1e-6)  # F1åˆ†æ•°ä½œä¸ºç®€åŒ–AP
        aps.append(ap)

    map_score = sum(aps) / len(aps) if aps else 0

    metrics = {
        'mAP': map_score,
        'AP_per_class': {class_names[i]: ap for i, ap in enumerate(aps)},
        'num_predictions': len(all_predictions),
        'num_targets': len(all_targets)
    }

    return metrics

# ---------------------------æ¨ç†----------------------------------

def predict_single_image(image_path, output_csv_path, model_path=None, class_names=None, device=None,
                         confidence_threshold=0.3):
    """
    å¯¹å•å¼ å›¾åƒè¿›è¡Œé¢„æµ‹å¹¶ä¿å­˜ç»“æœä¸ºCSV

    Args:
        image_path (str): è¾“å…¥å›¾åƒè·¯å¾„
        output_csv_path (str): è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        model_path (str, optional): æ¨¡å‹æƒé‡è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        class_names (list, optional): ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
        device (torch.device, optional): è®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
        confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼

    Returns:
        pd.DataFrame: åŒ…å«é¢„æµ‹ç»“æœçš„DataFrame
    """
    # è®¾ç½®é»˜è®¤å€¼
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    if class_names is None:
        class_names = ["wall", "door", "window", "column"]

    if model_path is None:
        model_path = "training_output/best_model.pth"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(image_path).exists():
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        return None

    if not Path(model_path).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None

    # åŠ è½½æ¨¡å‹
    model = DummyModel(num_classes=len(class_names)).to(device)
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
    output_dir = Path(output_csv_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    # è¿›è¡Œé¢„æµ‹
    model.eval()
    df = predict_and_save_csv(
        model=model,
        image_path=image_path,
        output_csv_path=output_csv_path,
        class_names=class_names,
        device=device,
        confidence_threshold=confidence_threshold
    )

    print(f"âœ… é¢„æµ‹å®Œæˆ! å…±æ£€æµ‹åˆ° {len(df)} ä¸ªç›®æ ‡")
    return df


def predict_multiple_images(image_dir, output_dir, model_path=None, class_names=None, device=None,
                            confidence_threshold=0.3):
    """
    æ‰¹é‡é¢„æµ‹ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒ

    Args:
        image_dir (str): è¾“å…¥å›¾åƒç›®å½•
        output_dir (str): è¾“å‡ºç›®å½•
        model_path (str, optional): æ¨¡å‹æƒé‡è·¯å¾„
        class_names (list, optional): ç±»åˆ«åç§°åˆ—è¡¨
        device (torch.device, optional): è®¾å¤‡
        confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼

    Returns:
        dict: æ¯ä¸ªå›¾åƒçš„é¢„æµ‹ç»“æœDataFrameå­—å…¸
    """
    # è®¾ç½®é»˜è®¤å€¼
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    if class_names is None:
        class_names = ["wall", "door", "window", "column"]

    if model_path is None:
        model_path = "training_output/best_model.pth"

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not Path(image_dir).exists():
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {image_dir}")
        return {}

    if not Path(model_path).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return {}

    # åŠ è½½æ¨¡å‹
    model = DummyModel(num_classes=len(class_names)).to(device)
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return {}

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
    image_paths = []
    for ext in image_extensions:
        image_paths.extend(Path(image_dir).glob(ext))
        image_paths.extend(Path(image_dir).glob(ext.upper()))

    if not image_paths:
        print(f"âŒ åœ¨ç›®å½• {image_dir} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        return {}

    print(f"ğŸ” æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒï¼Œå¼€å§‹æ‰¹é‡é¢„æµ‹...")

    # æ‰¹é‡é¢„æµ‹
    results = {}
    for i, image_path in enumerate(image_paths):
        output_csv_path = output_dir / f"{image_path.stem}_pred.csv"

        try:
            df = predict_and_save_csv(
                model=model,
                image_path=image_path,
                output_csv_path=output_csv_path,
                class_names=class_names,
                device=device,
                confidence_threshold=confidence_threshold
            )
            results[image_path.name] = df
            print(f"[{i + 1}/{len(image_paths)}] {image_path.name} -> æ£€æµ‹åˆ° {len(df)} ä¸ªç›®æ ‡")
        except Exception as e:
            print(f"âŒ é¢„æµ‹ {image_path.name} å¤±è´¥: {e}")
            results[image_path.name] = None

    successful_predictions = sum(1 for v in results.values() if v is not None)
    print(f"ğŸ‰ æ‰¹é‡é¢„æµ‹å®Œæˆ! æˆåŠŸå¤„ç† {successful_predictions}/{len(image_paths)} å¼ å›¾åƒ")

    return results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å•å¼ å›¾åƒé¢„æµ‹
    image_path = "data/pred_images/aaa.png"
    output_csv = "data/pred_images/aaa.csv"

    result_df = predict_single_image(
        image_path=image_path,
        output_csv_path=output_csv,
        model_path="training_output/best_model.pth",  # å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ training_output/best_model.pth
        class_names=["wall", "door", "window", "column"],  # å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨è¿™ä¸ªåˆ—è¡¨
        confidence_threshold=0.3  # å¯é€‰ï¼Œé»˜è®¤0.3
    )

    if result_df is not None:
        print("é¢„æµ‹ç»“æœé¢„è§ˆ:")
        print(result_df.head())

    # æ‰¹é‡é¢„æµ‹
    image_dir = "test_images/"
    output_dir = "batch_predictions/"

    batch_results = predict_multiple_images(
        image_dir=image_dir,
        output_dir=output_dir,
        model_path="training_output/best_model.pth",
        confidence_threshold=0.3
    )

    # ç»Ÿè®¡æ‰¹é‡é¢„æµ‹ç»“æœ
    total_detections = 0
    for image_name, df in batch_results.items():
        if df is not None:
            total_detections += len(df)

    print(f"æ‰¹é‡é¢„æµ‹æ€»è®¡æ£€æµ‹åˆ° {total_detections} ä¸ªç›®æ ‡")