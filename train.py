# train.py
import torch
from data.detection_dataset import DetectionDataset, custom_collate_fn
from loss import detection_loss
from models.default_model import DummyModel
from torch.utils.data import DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import time
import json
from pathlib import Path

from models.sta_adapter import DINOv3STADetector
from validate import validate_model, calculate_metrics
from torch.utils.tensorboard import SummaryWriter  # æ–°å¢ï¼šç”¨äºå¯è§†åŒ–
from config import ModelConfig as config

def train_one_epoch(model, dataloader, optimizer, device, epoch, print_freq=10, stride = 16):
    model.train()
    total_loss = 0
    hm_loss_total = 0
    wh_loss_total = 0
    off_loss_total = 0
    class_names = ["wall", "door", "window", "column"]
    for batch_idx, batch  in enumerate(dataloader):
        # è§£åŒ…batch
        images, targets = batch

        # å°†imagesç§»åŠ¨åˆ°è®¾å¤‡ - ç°åœ¨imagesæ˜¯å¼ é‡
        images = images.to(device)
        # targetså·²ç»æ˜¯å­—å…¸åˆ—è¡¨ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†

        optimizer.zero_grad()

        outputs = model(images)
        loss, loss_dict = detection_loss(outputs, targets, num_classes=len(class_names), stride = stride)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss_dict["total"].item()
        hm_loss_total += loss_dict["hm"].item()
        wh_loss_total += loss_dict["wh"].item()
        off_loss_total += loss_dict["off"].item()

        if batch_idx % print_freq == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}/{len(dataloader)}, "
                  f"Loss: {loss_dict['total'].item():.4f} "
                  f"(hm: {loss_dict['hm'].item():.4f}, "
                  f"wh: {loss_dict['wh'].item():.4f}, "
                  f"off: {loss_dict['off'].item():.4f})")

    avg_loss = total_loss / len(dataloader)
    avg_hm = hm_loss_total / len(dataloader)
    avg_wh = wh_loss_total / len(dataloader)
    avg_off = off_loss_total / len(dataloader)

    return {
        "total": avg_loss,
        "hm": avg_hm,
        "wh": avg_wh,
        "off": avg_off
    }


def save_checkpoint(model, optimizer, scheduler, epoch, loss, path):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
        'timestamp': time.time()
    }
    torch.save(checkpoint, path)
    print(f"âœ… æ£€æŸ¥ç‚¹ä¿å­˜: {path}")



def freeze_backbone(model):
    """å†»ç»“ä¸»å¹²ç½‘ç»œ"""
    for name, param in model.named_parameters():
        if 'backbone' in name:
            param.requires_grad = False
            print(f"å†»ç»“å±‚: {name}")

def unfreeze_backbone(model):
    """è§£å†»ä¸»å¹²ç½‘ç»œ"""
    for name, param in model.named_parameters():
        if 'backbone' in name:
            param.requires_grad = True
            print(f"è§£å†»å±‚: {name}")

def main():
    # é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    class_names = ["wall", "door", "window", "column"]
    num_classes = len(class_names)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("training_output")
    output_dir.mkdir(exist_ok=True)
    best_model_dir = Path("training_output")
    best_model_dir.mkdir(exist_ok=True)
    # æ–°å¢ï¼šTensorBoardæ—¥å¿—è®°å½•å™¨
    writer = SummaryWriter(log_dir=output_dir / "tensorboard_logs")
    # æ•°æ®é›†
    train_dataset = DetectionDataset(
        img_dir="data/cvt_images",
        csv_dir="data/cvt_images",
        class_names=class_names,
        training=True
    )
    has_validation = True
    # éªŒè¯é›†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    val_dataset = DetectionDataset(
        img_dir="data/val_images",  # ä½ éœ€è¦å‡†å¤‡éªŒè¯é›†
        csv_dir="data/val_images",
        class_names=class_names,
        training=False
    )

    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0,
                              collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0,
                            collate_fn=custom_collate_fn)

    # æ¨¡å‹ï¼ˆå…ˆç”¨ä½ çš„DummyModelæµ‹è¯•ï¼Œåç»­æ›¿æ¢ä¸ºDINOv3ï¼‰
    stride = config.stride
    # resnet backbone
    #model = DummyModel(num_classes=num_classes, stride = stride).to(device)
    # dinov3 backbone
    model =DINOv3STADetector(num_classes=4).to(device)
    # æ‰“å°æ€»å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    is_only_val = False
    if is_only_val:
        model.load_state_dict(torch.load("training_output/best_detect_model.pth", map_location=device))
        model.eval()
        print("å¼€å§‹éªŒè¯...")
        val_losses = validate_model(model, val_loader, device, num_classes, stride=stride)
        print(f"éªŒè¯æŸå¤± - æ€»: {val_losses['total']:.4f}, "
              f"çƒ­åŠ›å›¾: {val_losses['hm']:.4f}, "
              f"å®½é«˜: {val_losses['wh']:.4f}, "
              f"åç§»: {val_losses['off']:.4f}")
        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_metrics(model, val_loader, device, class_names, stride=stride)
        print(f"éªŒè¯æŒ‡æ ‡ - mAP: {metrics['mAP']:.4f}")
        for class_name, ap in metrics['AP_per_class'].items():
            print(f"  {class_name} AP: {ap:.4f}")

    # # å†»ç»“ä¸»å¹²ç½‘ç»œ
    # for param in model.backbone.parameters():
    #     param.requires_grad = False
    # ä¼˜åŒ–å™¨
    #optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)
    # optimizer = torch.optim.AdamW([
    #     {'params': model.backbone.parameters(), 'lr': 1e-5},  # æä½çš„å­¦ä¹ ç‡
    #     {'params': model.heatmap_head.parameters(), 'lr': 1e-4},
    #     {'params': model.wh_head.parameters(), 'lr': 1e-4},
    #     {'params': model.offset_head.parameters(), 'lr': 1e-4},
    # ], weight_decay=1e-3)
    max_epoch = 300
    phase1_epochs = 0  # é˜¶æ®µ1ï¼šå†»ç»“ä¸»å¹²ï¼Œè®­ç»ƒå¤´
    phase2_epochs = 300  # é˜¶æ®µ2ï¼šè§£å†»ä¸»å¹²ï¼Œå°å­¦ä¹ ç‡
    #scheduler = CosineAnnealingLR(optimizer, T_max=max_epoch, eta_min=1e-6)
    # é˜¶æ®µ1ï¼šå†»ç»“ä¸»å¹²ç½‘ç»œ
    print("=== é˜¶æ®µ1ï¼šå†»ç»“ä¸»å¹²ç½‘ç»œï¼Œåªè®­ç»ƒæ£€æµ‹å¤´ ===")
    freeze_backbone(model)

    # ä¼˜åŒ–å™¨ - é˜¶æ®µ1åªä¼˜åŒ–æ£€æµ‹å¤´
    head_params = []
    for name, param in model.named_parameters():
        if 'backbone' not in name and param.requires_grad:
            head_params.append(param)

    optimizer = optim.AdamW(head_params, lr=1e-4, weight_decay=1e-3)
    scheduler = CosineAnnealingLR(optimizer, T_max=phase1_epochs, eta_min=1e-6)
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    train_history = []
    val_interval = 5  # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
    for epoch in range(max_epoch):
        print(f"\n--- Epoch {epoch + 1}/{max_epoch} ---")
        # é˜¶æ®µåˆ‡æ¢é€»è¾‘
        if epoch == phase1_epochs:
            print("=== é˜¶æ®µ2ï¼šè§£å†»ä¸»å¹²ç½‘ç»œï¼Œå®Œæ•´å¾®è°ƒ ===")
            unfreeze_backbone(model)
            # é‡æ–°å®šä¹‰ä¼˜åŒ–å™¨ï¼ŒåŒ…å«æ‰€æœ‰å‚æ•°ä½†ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
            backbone_params = []
            head_params = []
            for name, param in model.named_parameters():
                if 'backbone' in name:
                    backbone_params.append(param)
                else:
                    head_params.append(param)
            optimizer = optim.AdamW([
                {'params': backbone_params, 'lr': 1e-4},  # ä¸»å¹²ç½‘ç»œç”¨å°å­¦ä¹ ç‡
                {'params': head_params, 'lr': 1e-4}  # æ£€æµ‹å¤´ç”¨è¾ƒå¤§å­¦ä¹ ç‡
            ], weight_decay=1e-3)
            scheduler = CosineAnnealingLR(optimizer, T_max=phase2_epochs, eta_min=1e-6)
            # å¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«
            # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            #     optimizer,
            #     T_0=10,  # 10ä¸ªepochåé‡å¯
            #     T_mult=2,
            #     eta_min=1e-6
            # )
        # è®­ç»ƒ
        train_losses = train_one_epoch(model, train_loader, optimizer, device, epoch + 1, stride = stride)
        print(f"è®­ç»ƒæŸå¤± - æ€»: {train_losses['total']:.4f}, "
              f"çƒ­åŠ›å›¾: {train_losses['hm']:.4f}, "
              f"å®½é«˜: {train_losses['wh']:.4f}, "
              f"åç§»: {train_losses['off']:.4f}")
        # æ–°å¢ï¼šè®°å½•è®­ç»ƒæŸå¤±åˆ°TensorBoard
        writer.add_scalar('Loss/train_total', train_losses['total'], epoch)
        writer.add_scalar('Loss/train_hm', train_losses['hm'], epoch)
        writer.add_scalar('Loss/train_wh', train_losses['wh'], epoch)
        writer.add_scalar('Loss/train_off', train_losses['off'], epoch)
        # è®°å½•å­¦ä¹ ç‡
        current_lr = scheduler.get_last_lr()[0] if epoch < phase1_epochs else optimizer.param_groups[0]['lr']
        writer.add_scalar('LearningRate', current_lr, epoch)
        # éªŒè¯
        if has_validation and (epoch + 1) % val_interval == 0:
            print("å¼€å§‹éªŒè¯...")
            val_losses = validate_model(model, val_loader, device, num_classes, stride = stride)
            print(f"éªŒè¯æŸå¤± - æ€»: {val_losses['total']:.4f}, "
                  f"çƒ­åŠ›å›¾: {val_losses['hm']:.4f}, "
                  f"å®½é«˜: {val_losses['wh']:.4f}, "
                  f"åç§»: {val_losses['off']:.4f}")
            # æ–°å¢ï¼šè®°å½•éªŒè¯æŸå¤±åˆ°TensorBoard
            writer.add_scalar('Loss/val_total', val_losses['total'], epoch)
            writer.add_scalar('Loss/val_hm', val_losses['hm'], epoch)
            writer.add_scalar('Loss/val_wh', val_losses['wh'], epoch)
            writer.add_scalar('Loss/val_off', val_losses['off'], epoch)
            # è®¡ç®—æŒ‡æ ‡
            metrics = calculate_metrics(model, val_loader, device, class_names, stride = stride)
            print(f"éªŒè¯æŒ‡æ ‡ - mAP: {metrics['mAP']:.4f}")
            writer.add_scalar('Metrics/mAP', metrics['mAP'], epoch)
            for class_name, ap in metrics['AP_per_class'].items():
                print(f"  {class_name} AP: {ap:.4f}")
                writer.add_scalar(f'Metrics/AP_{class_name}', ap, epoch)
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # è®°å½•å†å²
        train_history.append({
            'epoch': epoch + 1,
            'train_loss': train_losses,
            'lr': scheduler.get_last_lr()[0]
        })

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 10 == 0:
            checkpoint_path = output_dir / f"checkpoint_epoch_{epoch + 1}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': train_losses,
            }, checkpoint_path)
            print(f"âœ… æ£€æŸ¥ç‚¹ä¿å­˜: {checkpoint_path}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        current_loss = train_losses['total']
        if has_validation and (epoch + 1) % val_interval == 0:
            current_loss = val_losses['total']  # ä½¿ç”¨éªŒè¯æŸå¤±é€‰æ‹©æœ€ä½³æ¨¡å‹

        if current_loss < best_loss:
            best_loss = current_loss
            best_model_path = best_model_dir / "best_detect_model.pth"
            torch.save(model.state_dict(), best_model_path)
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! Loss: {best_loss:.4f}")

        # ä¿å­˜è®­ç»ƒå†å²
        with open(output_dir / "training_history.json", 'w') as f:
            json.dump(train_history, f, indent=2)
    # æ–°å¢ï¼šå…³é—­TensorBoardå†™å…¥å™¨
    writer.close()
    print("è®­ç»ƒå®Œæˆï¼ä½¿ç”¨ 'tensorboard --logdir training_output/tensorboard_logs' æŸ¥çœ‹è®­ç»ƒæ›²çº¿")

if __name__ == "__main__":
    import sys
    import torch
    import torchvision
    import numpy as np
    print("Pythonç‰ˆæœ¬:", sys.version)
    print("PyTorchç‰ˆæœ¬:", torch.__version__)
    print("Torchvisionç‰ˆæœ¬:", torchvision.__version__)
    print("CUDAæ˜¯å¦å¯ç”¨:", torch.cuda.is_available())
    print("CUDAç‰ˆæœ¬:", torch.version.cuda)
    print("numpyç‰ˆæœ¬:", np.__version__)
    print("è®¾å¤‡:", torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    main()