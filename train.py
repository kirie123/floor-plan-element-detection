# train.py
import torch
from data.detection_dataset import DetectionDataset
from loss import detection_loss

from models.default_model import DummyModel


from torch.utils.data import DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import time
import json
from pathlib import Path

from validate import validate_model, calculate_metrics


def train_one_epoch(model, dataloader, optimizer, device, epoch, print_freq=10, stride = 16):
    model.train()
    total_loss = 0
    hm_loss_total = 0
    wh_loss_total = 0
    off_loss_total = 0
    class_names = ["wall", "door", "window", "column"]
    for batch_idx, batch  in enumerate(dataloader):
        # è§£åŒ…batch
        images, targets = batch

        # å°†imagesç§»åŠ¨åˆ°è®¾å¤‡ - ç°åœ¨imagesæ˜¯å¼ é‡
        images = images.to(device)
        # targetså·²ç»æ˜¯å­—å…¸åˆ—è¡¨ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†

        optimizer.zero_grad()

        outputs = model(images)
        loss, loss_dict = detection_loss(outputs, targets, num_classes=len(class_names), stride = stride)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss_dict["total"].item()
        hm_loss_total += loss_dict["hm"].item()
        wh_loss_total += loss_dict["wh"].item()
        off_loss_total += loss_dict["off"].item()

        if batch_idx % print_freq == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}/{len(dataloader)}, "
                  f"Loss: {loss_dict['total'].item():.4f} "
                  f"(hm: {loss_dict['hm'].item():.4f}, "
                  f"wh: {loss_dict['wh'].item():.4f}, "
                  f"off: {loss_dict['off'].item():.4f})")

    avg_loss = total_loss / len(dataloader)
    avg_hm = hm_loss_total / len(dataloader)
    avg_wh = wh_loss_total / len(dataloader)
    avg_off = off_loss_total / len(dataloader)

    return {
        "total": avg_loss,
        "hm": avg_hm,
        "wh": avg_wh,
        "off": avg_off
    }


def save_checkpoint(model, optimizer, scheduler, epoch, loss, path):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
        'timestamp': time.time()
    }
    torch.save(checkpoint, path)
    print(f"âœ… æ£€æŸ¥ç‚¹ä¿å­˜: {path}")

def custom_collate_fn(batch):
    """
    ç®€å•çš„æ•´ç†å‡½æ•°ï¼Œå°†å›¾åƒå †å ï¼Œç›®æ ‡ä¿æŒä¸ºåˆ—è¡¨
    """
    images = []
    targets = []

    for image, target in batch:
        images.append(image)
        targets.append(target)

    # å †å å›¾åƒ
    images = torch.stack(images, dim=0)

    return images, targets
def main():
    # é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    class_names = ["wall", "door", "window", "column"]
    num_classes = len(class_names)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("training_output")
    output_dir.mkdir(exist_ok=True)

    # æ•°æ®é›†
    train_dataset = DetectionDataset(
        img_dir="data/cvt_images",
        csv_dir="data/cvt_images",
        class_names=class_names,
        training=True
    )
    has_validation = True
    # éªŒè¯é›†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    val_dataset = DetectionDataset(
        img_dir="data/val_images",  # ä½ éœ€è¦å‡†å¤‡éªŒè¯é›†
        csv_dir="data/val_images",
        class_names=class_names,
        training=False
    )

    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0,
                              collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0,
                            collate_fn=custom_collate_fn)

    # æ¨¡å‹ï¼ˆå…ˆç”¨ä½ çš„DummyModelæµ‹è¯•ï¼Œåç»­æ›¿æ¢ä¸ºDINOv3ï¼‰
    stride = 16
    model = DummyModel(num_classes=num_classes, stride = stride).to(device)
    # # å†»ç»“ä¸»å¹²ç½‘ç»œ
    # for param in model.backbone.parameters():
    #     param.requires_grad = False
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)
    # optimizer = torch.optim.AdamW([
    #     {'params': model.backbone.parameters(), 'lr': 1e-5},  # æä½çš„å­¦ä¹ ç‡
    #     {'params': model.heatmap_head.parameters(), 'lr': 1e-4},
    #     {'params': model.wh_head.parameters(), 'lr': 1e-4},
    #     {'params': model.offset_head.parameters(), 'lr': 1e-4},
    # ], weight_decay=1e-3)
    max_epoch = 200
    scheduler = CosineAnnealingLR(optimizer, T_max=max_epoch, eta_min=1e-6)

    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    train_history = []
    val_interval = 5  # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
    for epoch in range(max_epoch):
        print(f"\n--- Epoch {epoch + 1}/{max_epoch} ---")

        # è®­ç»ƒ
        train_losses = train_one_epoch(model, train_loader, optimizer, device, epoch + 1, stride = stride)
        print(f"è®­ç»ƒæŸå¤± - æ€»: {train_losses['total']:.4f}, "
              f"çƒ­åŠ›å›¾: {train_losses['hm']:.4f}, "
              f"å®½é«˜: {train_losses['wh']:.4f}, "
              f"åç§»: {train_losses['off']:.4f}")

        # éªŒè¯
        if has_validation and (epoch + 1) % val_interval == 0:
            print("å¼€å§‹éªŒè¯...")
            val_losses = validate_model(model, val_loader, device, num_classes, stride = stride)
            print(f"éªŒè¯æŸå¤± - æ€»: {val_losses['total']:.4f}, "
                  f"çƒ­åŠ›å›¾: {val_losses['hm']:.4f}, "
                  f"å®½é«˜: {val_losses['wh']:.4f}, "
                  f"åç§»: {val_losses['off']:.4f}")

            # è®¡ç®—æŒ‡æ ‡
            metrics = calculate_metrics(model, val_loader, device, class_names, stride = stride)
            print(f"éªŒè¯æŒ‡æ ‡ - mAP: {metrics['mAP']:.4f}")
            for class_name, ap in metrics['AP_per_class'].items():
                print(f"  {class_name} AP: {ap:.4f}")
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # è®°å½•å†å²
        train_history.append({
            'epoch': epoch + 1,
            'train_loss': train_losses,
            'lr': scheduler.get_last_lr()[0]
        })

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 5 == 0:
            checkpoint_path = output_dir / f"checkpoint_epoch_{epoch + 1}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': train_losses,
            }, checkpoint_path)
            print(f"âœ… æ£€æŸ¥ç‚¹ä¿å­˜: {checkpoint_path}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        current_loss = train_losses['total']
        if has_validation and (epoch + 1) % val_interval == 0:
            current_loss = val_losses['total']  # ä½¿ç”¨éªŒè¯æŸå¤±é€‰æ‹©æœ€ä½³æ¨¡å‹

        if current_loss < best_loss:
            best_loss = current_loss
            best_model_path = output_dir / "best_model.pth"
            torch.save(model.state_dict(), best_model_path)
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! Loss: {best_loss:.4f}")

        # ä¿å­˜è®­ç»ƒå†å²
        with open(output_dir / "training_history.json", 'w') as f:
            json.dump(train_history, f, indent=2)


if __name__ == "__main__":
    import sys
    import torch
    import torchvision
    import numpy as np
    print("Pythonç‰ˆæœ¬:", sys.version)
    print("PyTorchç‰ˆæœ¬:", torch.__version__)
    print("Torchvisionç‰ˆæœ¬:", torchvision.__version__)
    print("CUDAæ˜¯å¦å¯ç”¨:", torch.cuda.is_available())
    print("CUDAç‰ˆæœ¬:", torch.version.cuda)
    print("numpyç‰ˆæœ¬:", np.__version__)
    print("è®¾å¤‡:", torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    main()