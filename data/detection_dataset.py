import os

import pandas as pd
import torchvision.transforms as T
from pathlib import Path
import torch
from torch.utils.data import Dataset
import cv2
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
import random
from PIL import Image, ImageEnhance, ImageOps



class GridMask:
    """GridMaskæ•°æ®å¢å¼º - ç½‘æ ¼çŠ¶é®æŒ¡"""
    
    def __init__(self, use_h=True, use_w=True, rotate=1, offset=False, ratio=0.5, mode=0, prob=0.7):
        """
        Args:
            use_h (bool): æ˜¯å¦åœ¨é«˜åº¦æ–¹å‘ä½¿ç”¨ç½‘æ ¼
            use_w (bool): æ˜¯å¦åœ¨å®½åº¦æ–¹å‘ä½¿ç”¨ç½‘æ ¼
            rotate (float): æ—‹è½¬è§’åº¦èŒƒå›´
            offset (bool): æ˜¯å¦ä½¿ç”¨åç§»
            ratio (float): ç½‘æ ¼å¯†åº¦æ¯”ä¾‹
            mode (int): æ¨¡å¼ï¼Œ0: éšæœºï¼Œ1: å›ºå®š
            prob (float): åº”ç”¨æ¦‚ç‡
        """
        self.use_h = use_h
        self.use_w = use_w
        self.rotate = rotate
        self.offset = offset
        self.ratio = ratio
        self.mode = mode
        self.prob = prob
        
    def __call__(self, img, boxes=None, labels=None):
        """
        Args:
            img: è¾“å…¥å›¾åƒ [H, W, C]
            boxes: è¾¹ç•Œæ¡†åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
        
        Returns:
            å¢å¼ºåçš„å›¾åƒå’Œæ ‡æ³¨
        """
        if random.random() > self.prob:
            return img, boxes, labels
            
        h, w, c = img.shape
        
        # ç”Ÿæˆç½‘æ ¼æ©ç 
        mask = self.generate_grid_mask(h, w)
        
        # åº”ç”¨æ©ç 
        if self.mode == 1:
            # æ¨¡å¼1: ç›´æ¥åº”ç”¨ç½‘æ ¼
            img = img * mask
        else:
            # æ¨¡å¼0: éšæœºé€‰æ‹©ç½‘æ ¼å•å…ƒè¿›è¡Œé®æŒ¡
            img = self.apply_random_grid_mask(img, mask)
        
        return img, boxes, labels
    
    def generate_grid_mask(self, h, w):
        """ç”Ÿæˆç½‘æ ¼æ©ç """
        # è®¡ç®—ç½‘æ ¼å°ºå¯¸
        grid_h = int(h * self.ratio)
        grid_w = int(w * self.ratio)
        
        # ç¡®ä¿ç½‘æ ¼å°ºå¯¸åˆç†
        grid_h = max(8, min(grid_h, h // 4))
        grid_w = max(8, min(grid_w, w // 4))
        
        # åˆ›å»ºåŸºç¡€ç½‘æ ¼
        mask = np.ones((h, w), dtype=np.float32)
        
        if self.use_h:
            # åœ¨é«˜åº¦æ–¹å‘æ·»åŠ ç½‘æ ¼çº¿
            for i in range(0, h, grid_h):
                mask[i:min(i+2, h), :] = 0  # 2åƒç´ å®½çš„ç½‘æ ¼çº¿
        
        if self.use_w:
            # åœ¨å®½åº¦æ–¹å‘æ·»åŠ ç½‘æ ¼çº¿
            for j in range(0, w, grid_w):
                mask[:, j:min(j+2, w)] = 0
        
        # éšæœºæ—‹è½¬
        if self.rotate > 0:
            angle = random.uniform(-self.rotate, self.rotate)
            center = (w // 2, h // 2)
            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            mask = cv2.warpAffine(mask, rotation_matrix, (w, h), flags=cv2.INTER_NEAREST)
        
        # éšæœºåç§»
        if self.offset:
            dx = random.randint(-grid_w//2, grid_w//2)
            dy = random.randint(-grid_h//2, grid_h//2)
            translation_matrix = np.float32([[1, 0, dx], [0, 1, dy]])
            mask = cv2.warpAffine(mask, translation_matrix, (w, h), flags=cv2.INTER_NEAREST)
        
        return np.expand_dims(mask, axis=2)  # [H, W, 1]
    
    def apply_random_grid_mask(self, img, mask):
        """éšæœºåº”ç”¨ç½‘æ ¼æ©ç """
        h, w, c = img.shape
        
        # åˆ›å»ºéšæœºé€‰æ‹©æ©ç 
        random_mask = np.random.random((h, w, 1)) > 0.5
        
        # ç»„åˆæ©ç 
        final_mask = np.where(random_mask, mask, 1.0)
        
        # åº”ç”¨æ©ç 
        img = img * final_mask
        
        return img.astype(np.uint8)


def convert_class(cls_name):
    if 'WALL'in cls_name:
        return 'wall'
    return cls_name.lower()


class ColorAugmentation:
    """ä¸“é—¨é’ˆå¯¹CADå›¾çº¸çš„é¢œè‰²å¢å¼º"""

    @staticmethod
    def invert_background(image, prob=0.5):
        """èƒŒæ™¯åè½¬ï¼šç™½è‰²<->é»‘è‰²"""
        if random.random() < prob:
            # åè½¬å›¾åƒ
            inverted = cv2.bitwise_not(image)
            return inverted
        return image

    @staticmethod
    def change_background_color(image, prob=0.3):
        """æ”¹å˜èƒŒæ™¯é¢œè‰²"""
        if random.random() < prob:
            h, w, c = image.shape
            # éšæœºèƒŒæ™¯é¢œè‰²ï¼ˆå¸¸è§CADèƒŒæ™¯è‰²ï¼‰
            bg_colors = [
                [255, 255, 255],  # ç™½è‰²
                [0, 0, 0],  # é»‘è‰²
                [240, 240, 240],  # æµ…ç°è‰²
                [30, 30, 30],  # æ·±ç°è‰²
                [255, 250, 240],  # ç±³ç™½è‰²
                [245, 245, 245],  # çƒŸç™½è‰²
            ]

            bg_color = random.choice(bg_colors)
            # åˆ›å»ºæ–°èƒŒæ™¯
            new_bg = np.full_like(image, bg_color)

            # åŸºäºé˜ˆå€¼åˆ†ç¦»å‰æ™¯å’ŒèƒŒæ™¯
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

            # è‡ªé€‚åº”é˜ˆå€¼ï¼Œé€‚åº”ä¸åŒå¯¹æ¯”åº¦
            if random.random() < 0.5:
                _, mask = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
            else:
                mask = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                             cv2.THRESH_BINARY, 11, 2)

            # å¯¹maskè¿›è¡Œå½¢æ€å­¦æ“ä½œï¼Œæ¶ˆé™¤å™ªå£°
            kernel = np.ones((2, 2), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

            # åº”ç”¨mask
            foreground = cv2.bitwise_and(image, image, mask=mask)
            background = cv2.bitwise_and(new_bg, new_bg, mask=cv2.bitwise_not(mask))
            result = cv2.add(foreground, background)

            return result
        return image

    @staticmethod
    def adjust_line_color(image, prob=0.4):
        """è°ƒæ•´çº¿æ¡é¢œè‰²"""
        if random.random() < prob:
            # éšæœºçº¿æ¡é¢œè‰²å˜åŒ–
            line_colors = [
                lambda img: img,  # ä¿æŒåŸè‰²
                lambda img: cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB),  # ç°åº¦
                lambda img: ColorAugmentation.apply_sepia(img),  # æ£•è¤è‰²è°ƒ
                lambda img: ColorAugmentation.apply_blueprint(img),  # è“å›¾é£æ ¼
                lambda img: ColorAugmentation.apply_red_lines(img),  # çº¢è‰²çº¿æ¡
                lambda img: ColorAugmentation.apply_green_lines(img),  # ç»¿è‰²çº¿æ¡
            ]

            augment_func = random.choice(line_colors)
            return augment_func(image)
        return image

    @staticmethod
    def apply_sepia(image):
        """åº”ç”¨æ£•è¤è‰²è°ƒ"""
        sepia_filter = np.array([[0.393, 0.769, 0.189],
                                 [0.349, 0.686, 0.168],
                                 [0.272, 0.534, 0.131]])
        sepia_img = cv2.transform(image, sepia_filter)
        sepia_img = np.clip(sepia_img, 0, 255).astype(np.uint8)
        return sepia_img

    @staticmethod
    def apply_blueprint(image):
        """è“å›¾é£æ ¼ï¼ˆè“è‰²èƒŒæ™¯ï¼Œç™½è‰²/é»„è‰²çº¿æ¡ï¼‰"""
        h, w, c = image.shape
        # åˆ›å»ºè“è‰²èƒŒæ™¯
        blueprint_bg = np.array([30, 60, 120])  # æ·±è“è‰²

        # è½¬æ¢ä¸ºç°åº¦å¹¶äºŒå€¼åŒ–
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

        # åˆ›å»ºè“å›¾é£æ ¼
        result = np.full_like(image, blueprint_bg)

        # çº¿æ¡å¯ä»¥æ˜¯ç™½è‰²æˆ–é»„è‰²
        line_color = random.choice([np.array([255, 255, 255]), np.array([255, 255, 0])])
        result[binary > 127] = line_color

        return result

    @staticmethod
    def apply_red_lines(image):
        """çº¢è‰²çº¿æ¡é£æ ¼"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

        result = np.full_like(image, [255, 255, 255])  # ç™½è‰²èƒŒæ™¯
        result[binary > 127] = [255, 0, 0]  # çº¢è‰²çº¿æ¡

        return result

    @staticmethod
    def apply_green_lines(image):
        """ç»¿è‰²çº¿æ¡é£æ ¼"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

        result = np.full_like(image, [0, 0, 0])  # é»‘è‰²èƒŒæ™¯
        result[binary > 127] = [0, 255, 0]  # ç»¿è‰²çº¿æ¡

        return result

    @staticmethod
    def random_color_shift(image, prob=0.3):
        """éšæœºé¢œè‰²åç§»"""
        if random.random() < prob:
            h, w, c = image.shape
            # å¯¹æ¯ä¸ªé€šé“åº”ç”¨éšæœºåç§»
            for i in range(3):
                shift = random.randint(-20, 20)
                image[:, :, i] = np.clip(image[:, :, i].astype(np.int32) + shift, 0, 255)
        return image

    @staticmethod
    def adjust_contrast_brightness(image, prob=0.5):
        """è°ƒæ•´å¯¹æ¯”åº¦å’Œäº®åº¦"""
        if random.random() < prob:
            # éšæœºå¯¹æ¯”åº¦ (0.7 ~ 1.5)
            contrast = random.uniform(0.7, 1.5)
            # éšæœºäº®åº¦ (-30 ~ 30)
            brightness = random.randint(-30, 30)

            image = image.astype(np.float32)
            image = image * contrast + brightness
            image = np.clip(image, 0, 255).astype(np.uint8)

        return image

    @staticmethod
    def apply_all_color_augmentations(image):
        """åº”ç”¨æ‰€æœ‰é¢œè‰²å¢å¼ºï¼ˆæŒ‰é¡ºåºï¼‰"""
        aug_image = image.copy()

        # 1. èƒŒæ™¯é¢œè‰²å˜åŒ–
        aug_image = ColorAugmentation.change_background_color(aug_image, prob=0.3)

        # 2. èƒŒæ™¯åè½¬
        aug_image = ColorAugmentation.invert_background(aug_image, prob=0.3)

        # 3. çº¿æ¡é¢œè‰²è°ƒæ•´
        aug_image = ColorAugmentation.adjust_line_color(aug_image, prob=0.4)

        # 4. å¯¹æ¯”åº¦äº®åº¦è°ƒæ•´
        aug_image = ColorAugmentation.adjust_contrast_brightness(aug_image, prob=0.5)

        # 5. éšæœºé¢œè‰²åç§»
        aug_image = ColorAugmentation.random_color_shift(aug_image, prob=0.3)

        return aug_image

class DetectionDataset(Dataset):
    def __init__(self, img_dir: str, csv_dir: str, class_names: list, training=True, img_size=1024):
        """
        Args:
            img_dir: åˆ‡ç‰‡å›¾åƒç›®å½•
            csv_dir: å¯¹åº”CSVç›®å½•
            class_names: ç±»åˆ«åˆ—è¡¨
            training: è®­ç»ƒ/æµ‹è¯•æ¨¡å¼
        """
        self.img_dir = Path(img_dir)
        self.csv_dir = Path(csv_dir)
        self.class_names = class_names
        self.class_to_idx = {name: i for i, name in enumerate(class_names)}
        self.training = training
        self.img_size = img_size
        self.use_mosaic = training and True
        self.use_color_aug = training and True
        self.use_mixup = training and False
        self.mixup_alpha = 0.5  # MixUpå‚æ•°
        self.mosaic_prob = 0.75 # 75%æ¦‚ç‡ä½¿ç”¨Mosaicï¼Œå¯ä»¥æ ¹æ®æ•ˆæœè°ƒæ•´
        self.use_gridmask = training and True
        self.gridmask = GridMask(
            use_h=True,
            use_w=True,
            rotate=15,  # æ—‹è½¬è§’åº¦èŒƒå›´
            offset=True,  # ä½¿ç”¨åç§»
            ratio=0.6,  # ç½‘æ ¼å¯†åº¦
            mode=0,  # éšæœºæ¨¡å¼
            prob=0.99  # åº”ç”¨æ¦‚ç‡
        )
        # æ”¶é›†æ ·æœ¬
        self.samples = []
        for img_path in self.img_dir.glob("*.jpg"):
            csv_path = self.csv_dir / (img_path.stem + ".csv")
            if csv_path.exists():
                self.samples.append((img_path, csv_path))

        # ä½¿ç”¨albumentationsè¿›è¡Œæ•°æ®å¢å¼ºï¼ˆåŒæ­¥å¤„ç†å›¾åƒå’Œè¾¹ç•Œæ¡†ï¼‰
        self.default_transform = self.get_default_trasform()
        if training:
            self.transform = self.get_simple_transform()
            #self.transform = self.get_enhance_transform()
        else:
            self.transform = self.get_default_trasform()

        #self.transform = self.get_default_trasform()
        print(f"âœ… åŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")



    def apply_gridmask(self, image):
        """åº”ç”¨GridMaskå¢å¼º"""
        if self.training and self.use_gridmask:
            # ç¡®ä¿å›¾åƒæ˜¯uint8ç±»å‹
            if image.dtype != np.uint8:
                image = (image * 255).astype(np.uint8)
            # åº”ç”¨GridMask
            image, _, _ = self.gridmask(image)
            # ç¡®ä¿è¿”å›æ­£ç¡®çš„æ•°æ®ç±»å‹
            if image.dtype != np.uint8:
                image = image.astype(np.uint8)
        return image

    def resize_image_and_boxes(self, image, boxes, target_size):
        """
        æ‰‹åŠ¨è°ƒæ•´å›¾åƒå°ºå¯¸å’Œå¯¹åº”çš„è¾¹ç•Œæ¡†

        Args:
            image: åŸå§‹å›¾åƒ (H, W, C)
            boxes: è¾¹ç•Œæ¡†åˆ—è¡¨ [[xmin, ymin, xmax, ymax], ...]
            target_size: ç›®æ ‡å°ºå¯¸

        Returns:
            resized_image: è°ƒæ•´åçš„å›¾åƒ
            resized_boxes: è°ƒæ•´åçš„è¾¹ç•Œæ¡†
        """
        # è·å–åŸå§‹å°ºå¯¸
        orig_height, orig_width = image.shape[:2]

        # è°ƒæ•´å›¾åƒå°ºå¯¸
        resized_image = cv2.resize(image, (target_size, target_size))

        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale_x = target_size / orig_width
        scale_y = target_size / orig_height

        # è°ƒæ•´è¾¹ç•Œæ¡†åæ ‡
        resized_boxes = []
        for box in boxes:
            xmin, ymin, xmax, ymax = box
            # åº”ç”¨ç¼©æ”¾
            new_xmin = xmin * scale_x
            new_ymin = ymin * scale_y
            new_xmax = xmax * scale_x
            new_ymax = ymax * scale_y

            # ç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒèŒƒå›´å†…
            new_xmin = max(0, min(new_xmin, target_size - 1))
            new_ymin = max(0, min(new_ymin, target_size - 1))
            new_xmax = max(0, min(new_xmax, target_size - 1))
            new_ymax = max(0, min(new_ymax, target_size - 1))

            # åªä¿ç•™æœ‰æ•ˆçš„è¾¹ç•Œæ¡†
            if new_xmin < new_xmax and new_ymin < new_ymax:
                resized_boxes.append([new_xmin, new_ymin, new_xmax, new_ymax])

        return resized_image, resized_boxes
    def get_test_transform(self):
        return A.Compose([

                # 4. å¼¹æ€§å˜æ¢ï¼ˆæ¨¡æ‹Ÿå›¾çº¸å˜å½¢ï¼‰
                A.ElasticTransform(
                    alpha=30,
                    sigma=5,
                    alpha_affine=8,
                    p=0.2
                ),
                # è°ƒæ•´å°ºå¯¸åˆ°img_size
                A.Resize(self.img_size, self.img_size, always_apply=True),

                # æ ‡å‡†åŒ–å¹¶è½¬æ¢ä¸ºTensor
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], bbox_params=A.BboxParams(
                format='pascal_voc',  # ä½¿ç”¨[x_min, y_min, x_max, y_max]æ ¼å¼
                label_fields=['labels']
            ))
    def get_default_trasform(self):
        return A.Compose([
            # è°ƒæ•´å°ºå¯¸åˆ°img_size
            A.Resize(self.img_size, self.img_size, always_apply=True),
            # æ ‡å‡†åŒ–å¹¶è½¬æ¢ä¸ºTensor
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ], bbox_params=A.BboxParams(
            format='pascal_voc',  # ä½¿ç”¨[x_min, y_min, x_max, y_max]æ ¼å¼
            label_fields=['labels']
        ))
    def get_simple_transform(self):
        """é’ˆå¯¹å»ºç­‘å¹³é¢å›¾çš„ç®€åŒ–å¢å¼º"""
        return A.Compose([
                # é¢œè‰²å¢å¼º
                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
                A.OneOf([
                    # è½»å¾®çš„éå‡åŒ€ç¼©æ”¾
                    A.Affine(
                        scale={"x": (0.8, 1.8), "y": (0.8, 1.8)},  # æ¨ªçºµç‹¬ç«‹ç¼©æ”¾
                        translate_percent=(-0.05, 0.05),
                        rotate=(-10, 10),
                        shear=(-3, 3),
                        cval=255,  # å…³é”®ï¼šè®¾ç½®å¡«å……ä¸ºç™½è‰²
                        p=0.5
                    ),
                    # å‡åŒ€ç¼©æ”¾
                    A.Affine(
                        scale=(0.8, 1.2),
                        translate_percent=(-0.05, 0.05),
                        rotate=(-10, 10),
                        cval=255,  # å…³é”®ï¼šè®¾ç½®å¡«å……ä¸ºç™½è‰²
                        p=0.5
                    )
                ], p=0.3),  # 30%æ¦‚ç‡åº”ç”¨ç¼©æ”¾

                # å¼¹æ€§å˜æ¢ï¼ˆæ¨¡æ‹Ÿå›¾çº¸å˜å½¢ï¼‰
                A.ElasticTransform(
                    alpha=15,
                    sigma=3,
                    alpha_affine=5,
                    fill_value=255,  # å…³é”®ï¼šè®¾ç½®å¡«å……ä¸ºç™½è‰²
                    p=0.2
                ),
                # å‡ ä½•å˜æ¢ - è¿™äº›ä¼šåŒæ­¥è°ƒæ•´è¾¹ç•Œæ¡†
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.RandomRotate90(p=0.3),
                A.ShiftScaleRotate(
                    shift_limit=0.05,  # è½»å¾®å¹³ç§»
                    scale_limit=0.1,  # è½»å¾®ç¼©æ”¾
                    rotate_limit=5,  # å°è§’åº¦æ—‹è½¬
                    border_mode=cv2.BORDER_CONSTANT,  # ä½¿ç”¨å¸¸æ•°å¡«å……
                    value=255,  # å…³é”®ï¼šå¡«å……ç™½è‰²
                    p=0.5
                ),
                # ä¸“é—¨çš„é¢œè‰²å¢å¼º
                A.Lambda(name='ColorAugmentation',
                         image=lambda image, **kwargs: self.apply_color_augmentation(image),
                         p=0.4),  # 80%æ¦‚ç‡åº”ç”¨é¢œè‰²å¢å¼º

                # é’ˆå¯¹å»ºç­‘å¹³é¢å›¾çš„å¢å¼º
                A.GaussNoise(var_limit=(2.0, 10.0), p=0.2),
                #A.Blur(blur_limit=1, p=0.5),
                # åœ¨æ ‡å‡†åŒ–ä¹‹å‰æ·»åŠ GridMask
                A.Lambda(
                    image=lambda image, **kwargs: self.apply_gridmask(image),
                    p=0.5  # GridMaskçš„åº”ç”¨æ¦‚ç‡
                ),
                # è°ƒæ•´å°ºå¯¸åˆ°img_size
                A.Resize(self.img_size, self.img_size, always_apply=True),
                # æ ‡å‡†åŒ–å¹¶è½¬æ¢ä¸ºTensor
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], bbox_params=A.BboxParams(
                format='pascal_voc',  # ä½¿ç”¨[x_min, y_min, x_max, y_max]æ ¼å¼
                label_fields=['labels']
            ))

    def apply_color_augmentation(self, image, **kwargs):
        """åº”ç”¨è‡ªå®šä¹‰é¢œè‰²å¢å¼º"""
        if not self.training or not self.use_color_aug:
            return image

        # éšæœºé€‰æ‹©é¢œè‰²å¢å¼ºç­–ç•¥
        strategies = [
            #lambda img: ColorAugmentation.apply_all_color_augmentations(img),
            lambda img: ColorAugmentation.change_background_color(img, prob=0.5),
            lambda img: ColorAugmentation.invert_background(img, prob=0.5),
            lambda img: ColorAugmentation.adjust_line_color(img, prob=0.6),
            lambda img: ColorAugmentation.apply_blueprint(img),
            lambda img: img  # ä¿æŒåŸå›¾
        ]

        strategy = random.choice(strategies)
        augmented_image = strategy(image)

        return augmented_image

    def get_mixup_item(self, idx):
        """MixUpæ•°æ®å¢å¼º - ä¸¤å¼ å›¾åƒåˆ†åˆ«å¢å¼ºåæ··åˆ"""
        img_path1, csv_path1 = self.samples[idx]
        if "example" in str(img_path1):
            print(f"âš ï¸  è­¦å‘Šï¼šåŸå§‹å›¾åƒåŒ…å«'example'ï¼Œè·³è¿‡MixUp: {img_path1.name}")
            return self.load_single_sample(idx=idx,
                                           img_path=img_path1,
                                           csv_path=csv_path1, apply_transform=True)
        # éšæœºé€‰æ‹©å¦ä¸€å¼ å›¾åƒ
        max_attempts = 20
        attempts = 0
        while attempts < max_attempts:
            idx2 = random.randint(0, len(self.samples) - 1)
            if idx2 == idx:  # ç¡®ä¿ä¸æ˜¯åŒä¸€å¼ å›¾åƒ
                continue
            img_path2, csv_path2 = self.samples[idx2]
            # æ£€æŸ¥ç¬¬äºŒå¼ å›¾åƒè·¯å¾„æ˜¯å¦åŒ…å«"example"
            if "example" not in str(img_path2):
                break  # æ‰¾åˆ°åˆé€‚çš„å›¾åƒï¼Œè·³å‡ºå¾ªç¯
            attempts += 1
        # å¦‚æœå°è¯•å¤šæ¬¡åä»æœªæ‰¾åˆ°åˆé€‚çš„å›¾åƒï¼Œæ”¾å¼ƒMixUp
        if attempts >= max_attempts:
            print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•æ‰¾åˆ°ä¸åŒ…å«'example'çš„å›¾åƒè¿›è¡ŒMixUpï¼Œè¿”å›å•å¼ å›¾åƒ")
            return self.load_single_sample(idx=idx,
                                           img_path=img_path1,
                                           csv_path=csv_path1, apply_transform=True)
        # åŠ è½½ä¸¤å¼ å›¾åƒå¹¶åˆ†åˆ«åº”ç”¨å®Œæ•´çš„æ•°æ®å¢å¼º
        img_path1, csv_path1 = self.samples[idx]
        img_path2, csv_path2 = self.samples[idx2]
        # åˆ†åˆ«åº”ç”¨å®Œæ•´çš„æ•°æ®å¢å¼º
        image1, target1 = self.load_single_sample(idx=idx,
                                                  img_path=img_path1,
                                                  csv_path=csv_path1, apply_transform=False)
        image2, target2 = self.load_single_sample(idx=idx,
                                                  img_path=img_path2,
                                                  csv_path=csv_path2, apply_transform=False)
        # ç”Ÿæˆæ··åˆç³»æ•°
        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)
        # æ··åˆå›¾åƒï¼ˆç°åœ¨å›¾åƒå·²ç»æ˜¯tensoræ ¼å¼ï¼‰
        mixed_image = lam * image1 + (1 - lam) * image2
        # åˆå¹¶æ ‡ç­¾ - å¯¹äºç›®æ ‡æ£€æµ‹ï¼Œæˆ‘ä»¬åˆå¹¶ä¸¤å¼ å›¾åƒçš„æ‰€æœ‰è¾¹ç•Œæ¡†
        mixed_boxes = torch.cat([target1["boxes"], target2["boxes"]], dim=0)
        mixed_labels = torch.cat([target1["labels"], target2["labels"]], dim=0)
        # å¦‚æœæ²¡æœ‰è¾¹ç•Œæ¡†ï¼Œåˆ›å»ºè™šæ‹Ÿæ¡†
        if len(mixed_boxes) == 0:
            mixed_boxes = torch.tensor([[0, 0, 1, 1]], dtype=torch.float32)
            mixed_labels = torch.tensor([0], dtype=torch.int64)
        target = {
            "boxes": mixed_boxes,
            "labels": mixed_labels,
            "image_id": torch.tensor([idx]),
            "orig_size": torch.tensor([self.img_size, self.img_size]),
            "mixup_lambda": torch.tensor([lam])  # è®°å½•æ··åˆç³»æ•°
        }

        return mixed_image, target
    def get_valid_mixup_idx(self, idx):
        img_path, csv_path = self.samples[idx]
        if "example" in str(img_path):
            return False
        return True

    def get_cutmix_item_strict(self, idx):
        """CutMixå¢å¼º - ä¸¥æ ¼å¤„ç†è¾¹ç•Œæ¡†é‡å """
        img_path1, csv_path1 = self.samples[idx]
        idx2 = self.get_valid_mixup_idx(idx)
        img_path2, csv_path2 = self.samples[idx2]

        image1, target1 = self.load_single_sample(idx, img_path1, csv_path1, apply_transform=True)
        image2, target2 = self.load_single_sample(idx2, img_path2, csv_path2, apply_transform=True)

        # ç”Ÿæˆè£å‰ªåŒºåŸŸ
        h, w = image1.shape[1], image1.shape[2]
        lam = np.random.beta(0.3, 0.3)

        cut_ratio = np.sqrt(1. - lam)
        cut_w = int(w * cut_ratio)
        cut_h = int(h * cut_ratio)

        cx = np.random.randint(w)
        cy = np.random.randint(h)

        x1 = np.clip(cx - cut_w // 2, 0, w)
        y1 = np.clip(cy - cut_h // 2, 0, h)
        x2 = np.clip(cx + cut_w // 2, 0, w)
        y2 = np.clip(cy + cut_h // 2, 0, h)

        # æ‰§è¡ŒCutMix
        mixed_image = image1.clone()
        mixed_image[:, y1:y2, x1:x2] = image2[:, y1:y2, x1:x2]

        lam = 1 - ((x2 - x1) * (y2 - y1) / (w * h))

        # å¤„ç†å›¾åƒ1çš„è¾¹ç•Œæ¡† - è£å‰ªæ‰ä¸è£å‰ªåŒºåŸŸé‡å çš„éƒ¨åˆ†
        boxes1 = target1["boxes"]
        labels1 = target1["labels"]

        if len(boxes1) > 0:
            boxes1_pixel = boxes1 * self.img_size
            cropped_boxes1, cropped_labels1 = self.clip_boxes_outside_region(boxes1_pixel, labels1, x1, y1, x2, y2)
            cropped_boxes1 = cropped_boxes1 / self.img_size
        else:
            cropped_boxes1 = boxes1
            cropped_labels1 = labels1

        # å¤„ç†å›¾åƒ2çš„è¾¹ç•Œæ¡† - åªä¿ç•™åœ¨è£å‰ªåŒºåŸŸå†…çš„éƒ¨åˆ†
        boxes2 = target2["boxes"]
        labels2 = target2["labels"]

        if len(boxes2) > 0:
            boxes2_pixel = boxes2 * self.img_size
            clipped_boxes2, clipped_labels2 = self.clip_boxes_inside_region(boxes2_pixel, labels2, x1, y1, x2, y2)

            if len(clipped_boxes2) > 0:
                # è°ƒæ•´åæ ‡åˆ°æ··åˆå›¾åƒä¸­çš„ä½ç½®
                clipped_boxes2[:, 0] = clipped_boxes2[:, 0] - x1 + x1
                clipped_boxes2[:, 1] = clipped_boxes2[:, 1] - y1 + y1
                clipped_boxes2[:, 2] = clipped_boxes2[:, 2] - x1 + x1
                clipped_boxes2[:, 3] = clipped_boxes2[:, 3] - y1 + y1
                clipped_boxes2 = clipped_boxes2 / self.img_size
        else:
            clipped_boxes2 = boxes2
            clipped_labels2 = labels2

        # åˆå¹¶è¾¹ç•Œæ¡†
        mixed_boxes = torch.cat([cropped_boxes1, clipped_boxes2], dim=0)
        mixed_labels = torch.cat([cropped_labels1, clipped_labels2], dim=0)

        if len(mixed_boxes) == 0:
            mixed_boxes = torch.tensor([[0, 0, 1, 1]], dtype=torch.float32)
            mixed_labels = torch.tensor([0], dtype=torch.int64)

        target = {
            "boxes": mixed_boxes,
            "labels": mixed_labels,
            "image_id": torch.tensor([idx]),
            "orig_size": torch.tensor([self.img_size, self.img_size]),
            "cutmix_lambda": torch.tensor([lam])
        }

        return mixed_image, target

    def clip_boxes_outside_region(self, boxes, labels, x1, y1, x2, y2, min_area_ratio=0.3):
        """è£å‰ªè¾¹ç•Œæ¡†ï¼Œç§»é™¤ä¸æŒ‡å®šåŒºåŸŸé‡å çš„éƒ¨åˆ†"""
        if len(boxes) == 0:
            return boxes, labels

        clipped_boxes = []
        clipped_labels = []

        for i, box in enumerate(boxes):
            box_x1, box_y1, box_x2, box_y2 = box

            # è®¡ç®—ä¸è£å‰ªåŒºåŸŸçš„äº¤é›†
            inter_x1 = max(box_x1, x1)
            inter_y1 = max(box_y1, y1)
            inter_x2 = min(box_x2, x2)
            inter_y2 = min(box_y2, y2)

            # å¦‚æœæ²¡æœ‰äº¤é›†ï¼Œä¿ç•™æ•´ä¸ªè¾¹ç•Œæ¡†
            if inter_x1 >= inter_x2 or inter_y1 >= inter_y2:
                clipped_boxes.append(box.unsqueeze(0))
                clipped_labels.append(labels[i].unsqueeze(0))
                continue

            # è®¡ç®—åŸå§‹é¢ç§¯å’Œäº¤é›†é¢ç§¯
            orig_area = (box_x2 - box_x1) * (box_y2 - box_y1)
            inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)

            # å¦‚æœé‡å é¢ç§¯å¾ˆå°ï¼Œä¿ç•™æ•´ä¸ªè¾¹ç•Œæ¡†
            if inter_area / orig_area < 0.1:
                clipped_boxes.append(box.unsqueeze(0))
                clipped_labels.append(labels[i].unsqueeze(0))
                continue

            # å¦‚æœé‡å é¢ç§¯å¾ˆå¤§ï¼Œä¸¢å¼ƒæ•´ä¸ªè¾¹ç•Œæ¡†
            if inter_area / orig_area > min_area_ratio:
                continue

            # å¦åˆ™ï¼Œè£å‰ªè¾¹ç•Œæ¡†ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åˆ†å‰²ä¸ºå¤šä¸ªè¾¹ç•Œæ¡†ï¼‰
            # æˆ‘ä»¬é€‰æ‹©ä¿ç•™è¾ƒå¤§çš„éƒ¨åˆ†
            left_area = (x1 - box_x1) * (box_y2 - box_y1) if x1 > box_x1 else 0
            right_area = (box_x2 - x2) * (box_y2 - box_y1) if box_x2 > x2 else 0
            top_area = (box_x2 - box_x1) * (y1 - box_y1) if y1 > box_y1 else 0
            bottom_area = (box_x2 - box_x1) * (box_y2 - y2) if box_y2 > y2 else 0

            # é€‰æ‹©é¢ç§¯æœ€å¤§çš„éƒ¨åˆ†
            areas = [left_area, right_area, top_area, bottom_area]
            max_area_idx = areas.index(max(areas))

            if max_area_idx == 0 and left_area > 0:  # å·¦ä¾§éƒ¨åˆ†
                new_box = torch.tensor([box_x1, box_y1, x1, box_y2], dtype=torch.float32)
            elif max_area_idx == 1 and right_area > 0:  # å³ä¾§éƒ¨åˆ†
                new_box = torch.tensor([x2, box_y1, box_x2, box_y2], dtype=torch.float32)
            elif max_area_idx == 2 and top_area > 0:  # é¡¶éƒ¨éƒ¨åˆ†
                new_box = torch.tensor([box_x1, box_y1, box_x2, y1], dtype=torch.float32)
            elif max_area_idx == 3 and bottom_area > 0:  # åº•éƒ¨éƒ¨åˆ†
                new_box = torch.tensor([box_x1, y2, box_x2, box_y2], dtype=torch.float32)
            else:
                continue  # æ²¡æœ‰åˆé€‚çš„éƒ¨åˆ†ï¼Œä¸¢å¼ƒ

            clipped_boxes.append(new_box.unsqueeze(0))
            clipped_labels.append(labels[i].unsqueeze(0))

        if len(clipped_boxes) > 0:
            return torch.cat(clipped_boxes, dim=0), torch.cat(clipped_labels, dim=0)
        else:
            return torch.zeros((0, 4), dtype=torch.float32), torch.zeros((0,), dtype=torch.int64)

    def clip_boxes_inside_region(self, boxes, labels, x1, y1, x2, y2):
        """è£å‰ªè¾¹ç•Œæ¡†ï¼Œåªä¿ç•™åœ¨æŒ‡å®šåŒºåŸŸå†…çš„éƒ¨åˆ†"""
        if len(boxes) == 0:
            return boxes, labels

        clipped_boxes = []
        clipped_labels = []

        for i, box in enumerate(boxes):
            box_x1, box_y1, box_x2, box_y2 = box

            # è®¡ç®—ä¸è£å‰ªåŒºåŸŸçš„äº¤é›†
            inter_x1 = max(box_x1, x1)
            inter_y1 = max(box_y1, y1)
            inter_x2 = min(box_x2, x2)
            inter_y2 = min(box_y2, y2)

            # å¦‚æœæ²¡æœ‰äº¤é›†ï¼Œè·³è¿‡
            if inter_x1 >= inter_x2 or inter_y1 >= inter_y2:
                continue

            # ä½¿ç”¨äº¤é›†ä½œä¸ºæ–°çš„è¾¹ç•Œæ¡†
            new_box = torch.tensor([inter_x1, inter_y1, inter_x2, inter_y2], dtype=torch.float32)

            # æ£€æŸ¥æ–°è¾¹ç•Œæ¡†æ˜¯å¦æœ‰æ•ˆ
            if (inter_x2 - inter_x1) > 2 and (inter_y2 - inter_y1) > 2:  # æœ€å°å°ºå¯¸é™åˆ¶
                clipped_boxes.append(new_box.unsqueeze(0))
                clipped_labels.append(labels[i].unsqueeze(0))

        if len(clipped_boxes) > 0:
            return torch.cat(clipped_boxes, dim=0), torch.cat(clipped_labels, dim=0)
        else:
            return torch.zeros((0, 4), dtype=torch.float32), torch.zeros((0,), dtype=torch.int64)

    def __len__(self):
        return len(self.samples)

    def load_single_sample(self, idx, img_path, csv_path, apply_transform=True):
        # ä½¿ç”¨OpenCVåŠ è½½å›¾åƒï¼ˆalbumentationséœ€è¦numpyæ•°ç»„ï¼‰
        image = cv2.imread(str(img_path))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        orig_size = image.shape[:2]  # (H, W)

        # åŠ è½½æ ‡æ³¨
        df = pd.read_csv(csv_path)
        boxes = []
        labels = []

        for _, row in df.iterrows():
            # ä½¿ç”¨ç»å¯¹åæ ‡ï¼ˆä¸æ˜¯å½’ä¸€åŒ–çš„ï¼‰
            # albumentationsä¼šåœ¨å˜æ¢åè‡ªåŠ¨è°ƒæ•´è¾¹ç•Œæ¡†
            xmin = row['xmin']
            ymin = row['ymin']
            xmax = row['xmax']
            ymax = row['ymax']

            # ç¡®ä¿è¾¹ç•Œæ¡†æœ‰æ•ˆ
            if xmin < xmax and ymin < ymax:
                boxes.append([xmin, ymin, xmax, ymax])
                labels.append(self.class_to_idx[convert_class(row['class'])])

        if len(boxes) == 0:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆè¾¹ç•Œæ¡†ï¼Œåˆ›å»ºç©ºç›®æ ‡
            boxes = [[0, 0, 1, 1]]  # è™šæ‹Ÿå°æ¡†
            labels = [0]  # é»˜è®¤ç±»åˆ«

        # åº”ç”¨æ•°æ®å¢å¼ºï¼ˆåŒæ­¥å¤„ç†å›¾åƒå’Œè¾¹ç•Œæ¡†ï¼‰
        if self.transform is not None:
            if apply_transform:
                transformed = self.transform(
                    image=image,
                    bboxes=boxes,
                    labels=labels
                )
            else:
                transformed = self.default_transform(
                    image=image,
                    bboxes=boxes,
                    labels=labels
                )
            image = transformed['image']  # å·²ç»æ˜¯Tensor [3, H, W]
            boxes = transformed['bboxes']
            labels = transformed['labels']

        # è½¬æ¢è¾¹ç•Œæ¡†ä¸ºTensorå¹¶å½’ä¸€åŒ–
        if len(boxes) > 0:
            boxes = torch.tensor(boxes, dtype=torch.float32)
            # å½’ä¸€åŒ–åˆ° [0, 1]
            boxes[:, 0] = boxes[:, 0] / self.img_size  # xmin
            boxes[:, 1] = boxes[:, 1] / self.img_size  # ymin
            boxes[:, 2] = boxes[:, 2] / self.img_size  # xmax
            boxes[:, 3] = boxes[:, 3] / self.img_size  # ymax
        else:
            boxes = torch.zeros((0, 4), dtype=torch.float32)

        labels = torch.tensor(labels, dtype=torch.int64)

        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": torch.tensor([idx]),
            "orig_size": torch.tensor(orig_size)
        }

        return image, target

    def __getitem__(self, idx):
        # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ä¸”ä½¿ç”¨MixUpï¼Œéšæœºé€‰æ‹©å¦ä¸€å¼ å›¾åƒ
        if self.training and self.use_mixup and random.random() < 0.2:
            return self.get_cutmix_item_strict(idx)

        # å¦åˆ™è¿”å›å•å¼ å›¾åƒï¼ˆåº”ç”¨å®Œæ•´transformï¼‰
        img_path, csv_path = self.samples[idx]
        return self.load_single_sample(idx, img_path, csv_path, apply_transform=True)

    def tensor_to_numpy(self, tensor_image):
        """
        å°†æ ‡å‡†åŒ–åçš„tensorå›¾åƒè½¬æ¢å›numpyæ ¼å¼ç”¨äºä¿å­˜
        """
        # åæ ‡å‡†åŒ–
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        image = tensor_image * std + mean

        # è½¬æ¢ä¸ºnumpyå¹¶è°ƒæ•´èŒƒå›´åˆ°0-255
        image = image.mul(255).byte()
        image = image.numpy().transpose(1, 2, 0)  # CHW -> HWC

        return image

    def get_absolute_boxes(self, normalized_boxes):
        """
        å°†å½’ä¸€åŒ–çš„è¾¹ç•Œæ¡†åæ ‡è½¬æ¢å›ç»å¯¹åæ ‡
        """
        if len(normalized_boxes) == 0:
            return []

        boxes_abs = normalized_boxes.clone()
        boxes_abs[:, 0] = boxes_abs[:, 0] * self.img_size  # xmin
        boxes_abs[:, 1] = boxes_abs[:, 1] * self.img_size  # ymin
        boxes_abs[:, 2] = boxes_abs[:, 2] * self.img_size  # xmax
        boxes_abs[:, 3] = boxes_abs[:, 3] * self.img_size  # ymax

        return boxes_abs.numpy().astype(int)

    def save_boxes_to_csv(self, boxes, labels, csv_path, img_filename):
        """
        å°†è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ä¿å­˜ä¸ºCSVæ–‡ä»¶
        """
        data = []
        for i, (box, label_idx) in enumerate(zip(boxes, labels)):
            class_name = self.class_names[label_idx] if label_idx < len(self.class_names) else "unknown"
            data.append({
                'filename': img_filename,
                'xmin': box[0],
                'ymin': box[1],
                'xmax': box[2],
                'ymax': box[3],
                'class': class_name
            })

        df = pd.DataFrame(data)
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¾¹ç•Œæ¡†ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameä½†ä¿æŒåˆ—ç»“æ„
        if df.empty:
            df = pd.DataFrame(columns=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'class'])
        df.to_csv(csv_path, index=False)

    def save_augmented_samples(self,save_augmented_dir,  num_samples=50, start_idx=0):
        """
        ä¿å­˜ç»è¿‡æ•°æ®å¢å¼ºåçš„æ ·æœ¬åˆ°æŒ‡å®šæ–‡ä»¶å¤¹

        Args:
            num_samples: è¦ä¿å­˜çš„æ ·æœ¬æ•°é‡
            start_idx: èµ·å§‹ç´¢å¼•
        """
        if not save_augmented_dir:
            print("âŒ æœªè®¾ç½®ä¿å­˜ç›®å½•ï¼Œè¯·åˆå§‹åŒ–æ—¶æŒ‡å®š save_augmented_dir")
            return

        saved_count = 0
        idx = start_idx

        while saved_count < num_samples and idx < len(self.samples):
            try:
                # è·å–å¢å¼ºåçš„æ ·æœ¬
                image_tensor, target = self.__getitem__(idx)

                # è½¬æ¢å›¾åƒä¸ºå¯ä¿å­˜æ ¼å¼
                image_np = self.tensor_to_numpy(image_tensor)

                # è·å–è¾¹ç•Œæ¡†çš„ç»å¯¹åæ ‡
                boxes_abs = self.get_absolute_boxes(target["boxes"])
                labels = target["labels"].numpy()

                # ç”Ÿæˆæ–‡ä»¶å
                orig_name = self.samples[idx][0].stem
                img_filename = f"aug_{orig_name}_{saved_count:04d}.jpg"
                csv_filename = f"aug_{orig_name}_{saved_count:04d}.csv"
                img_path = os.path.join(save_augmented_dir, f"{img_filename}")
                csv_path = os.path.join(save_augmented_dir, f"{csv_filename}")

                # ä¿å­˜å›¾åƒ
                cv2.imwrite(str(img_path), cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))

                # ä¿å­˜CSVæ ‡æ³¨
                self.save_boxes_to_csv(boxes_abs, labels, csv_path, img_filename)

                saved_count += 1
                print(f"âœ… ä¿å­˜å¢å¼ºæ ·æœ¬ {saved_count}/{num_samples}: {img_filename}")

            except Exception as e:
                print(f"âŒ ä¿å­˜æ ·æœ¬ {idx} å¤±è´¥: {e}")

            idx += 1

        print(f"ğŸ‰ æˆåŠŸä¿å­˜ {saved_count} ä¸ªå¢å¼ºæ ·æœ¬åˆ° {save_augmented_dir}")

if __name__ == "__main__":
    class_names = ["wall", "door", "window", "column"]
    dataset = DetectionDataset(
        img_dir="cvt_images",
        csv_dir="cvt_images",
        class_names=class_names,
        training=True
    )
    save_augmented_dir = "vis_images"
    dataset.save_augmented_samples(save_augmented_dir=save_augmented_dir, num_samples=1000)