import os

import pandas as pd
import torchvision.transforms as T
from pathlib import Path
import torch
from torch.utils.data import Dataset
import cv2
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
import random
from PIL import Image, ImageEnhance, ImageOps



def convert_class(cls_name):
    if 'WALL'in cls_name:
        return 'wall'
    return cls_name.lower()


class ColorAugmentation:
    """ä¸“é—¨é’ˆå¯¹CADå›¾çº¸çš„é¢œè‰²å¢å¼º"""

    @staticmethod
    def invert_background(image, prob=0.5):
        """èƒŒæ™¯åè½¬ï¼šç™½è‰²<->é»‘è‰²"""
        if random.random() < prob:
            # åè½¬å›¾åƒ
            inverted = cv2.bitwise_not(image)
            return inverted
        return image

    @staticmethod
    def change_background_color(image, prob=0.3):
        """æ”¹å˜èƒŒæ™¯é¢œè‰²"""
        if random.random() < prob:
            h, w, c = image.shape
            # éšæœºèƒŒæ™¯é¢œè‰²ï¼ˆå¸¸è§CADèƒŒæ™¯è‰²ï¼‰
            bg_colors = [
                [255, 255, 255],  # ç™½è‰²
                [0, 0, 0],  # é»‘è‰²
                [240, 240, 240],  # æµ…ç°è‰²
                [30, 30, 30],  # æ·±ç°è‰²
                [255, 250, 240],  # ç±³ç™½è‰²
                [245, 245, 245],  # çƒŸç™½è‰²
            ]

            bg_color = random.choice(bg_colors)
            # åˆ›å»ºæ–°èƒŒæ™¯
            new_bg = np.full_like(image, bg_color)

            # åŸºäºé˜ˆå€¼åˆ†ç¦»å‰æ™¯å’ŒèƒŒæ™¯
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

            # è‡ªé€‚åº”é˜ˆå€¼ï¼Œé€‚åº”ä¸åŒå¯¹æ¯”åº¦
            if random.random() < 0.5:
                _, mask = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
            else:
                mask = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                             cv2.THRESH_BINARY, 11, 2)

            # å¯¹maskè¿›è¡Œå½¢æ€å­¦æ“ä½œï¼Œæ¶ˆé™¤å™ªå£°
            kernel = np.ones((2, 2), np.uint8)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

            # åº”ç”¨mask
            foreground = cv2.bitwise_and(image, image, mask=mask)
            background = cv2.bitwise_and(new_bg, new_bg, mask=cv2.bitwise_not(mask))
            result = cv2.add(foreground, background)

            return result
        return image

    @staticmethod
    def adjust_line_color(image, prob=0.4):
        """è°ƒæ•´çº¿æ¡é¢œè‰²"""
        if random.random() < prob:
            # éšæœºçº¿æ¡é¢œè‰²å˜åŒ–
            line_colors = [
                lambda img: img,  # ä¿æŒåŸè‰²
                lambda img: cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB),  # ç°åº¦
                lambda img: ColorAugmentation.apply_sepia(img),  # æ£•è¤è‰²è°ƒ
                lambda img: ColorAugmentation.apply_blueprint(img),  # è“å›¾é£æ ¼
                lambda img: ColorAugmentation.apply_red_lines(img),  # çº¢è‰²çº¿æ¡
                lambda img: ColorAugmentation.apply_green_lines(img),  # ç»¿è‰²çº¿æ¡
            ]

            augment_func = random.choice(line_colors)
            return augment_func(image)
        return image

    @staticmethod
    def apply_sepia(image):
        """åº”ç”¨æ£•è¤è‰²è°ƒ"""
        sepia_filter = np.array([[0.393, 0.769, 0.189],
                                 [0.349, 0.686, 0.168],
                                 [0.272, 0.534, 0.131]])
        sepia_img = cv2.transform(image, sepia_filter)
        sepia_img = np.clip(sepia_img, 0, 255).astype(np.uint8)
        return sepia_img

    @staticmethod
    def apply_blueprint(image):
        """è“å›¾é£æ ¼ï¼ˆè“è‰²èƒŒæ™¯ï¼Œç™½è‰²/é»„è‰²çº¿æ¡ï¼‰"""
        h, w, c = image.shape
        # åˆ›å»ºè“è‰²èƒŒæ™¯
        blueprint_bg = np.array([30, 60, 120])  # æ·±è“è‰²

        # è½¬æ¢ä¸ºç°åº¦å¹¶äºŒå€¼åŒ–
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

        # åˆ›å»ºè“å›¾é£æ ¼
        result = np.full_like(image, blueprint_bg)

        # çº¿æ¡å¯ä»¥æ˜¯ç™½è‰²æˆ–é»„è‰²
        line_color = random.choice([np.array([255, 255, 255]), np.array([255, 255, 0])])
        result[binary > 127] = line_color

        return result

    @staticmethod
    def apply_red_lines(image):
        """çº¢è‰²çº¿æ¡é£æ ¼"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

        result = np.full_like(image, [255, 255, 255])  # ç™½è‰²èƒŒæ™¯
        result[binary > 127] = [255, 0, 0]  # çº¢è‰²çº¿æ¡

        return result

    @staticmethod
    def apply_green_lines(image):
        """ç»¿è‰²çº¿æ¡é£æ ¼"""
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

        result = np.full_like(image, [0, 0, 0])  # é»‘è‰²èƒŒæ™¯
        result[binary > 127] = [0, 255, 0]  # ç»¿è‰²çº¿æ¡

        return result

    @staticmethod
    def random_color_shift(image, prob=0.3):
        """éšæœºé¢œè‰²åç§»"""
        if random.random() < prob:
            h, w, c = image.shape
            # å¯¹æ¯ä¸ªé€šé“åº”ç”¨éšæœºåç§»
            for i in range(3):
                shift = random.randint(-20, 20)
                image[:, :, i] = np.clip(image[:, :, i].astype(np.int32) + shift, 0, 255)
        return image

    @staticmethod
    def adjust_contrast_brightness(image, prob=0.5):
        """è°ƒæ•´å¯¹æ¯”åº¦å’Œäº®åº¦"""
        if random.random() < prob:
            # éšæœºå¯¹æ¯”åº¦ (0.7 ~ 1.5)
            contrast = random.uniform(0.7, 1.5)
            # éšæœºäº®åº¦ (-30 ~ 30)
            brightness = random.randint(-30, 30)

            image = image.astype(np.float32)
            image = image * contrast + brightness
            image = np.clip(image, 0, 255).astype(np.uint8)

        return image

    @staticmethod
    def apply_all_color_augmentations(image):
        """åº”ç”¨æ‰€æœ‰é¢œè‰²å¢å¼ºï¼ˆæŒ‰é¡ºåºï¼‰"""
        aug_image = image.copy()

        # 1. èƒŒæ™¯é¢œè‰²å˜åŒ–
        aug_image = ColorAugmentation.change_background_color(aug_image, prob=0.3)

        # 2. èƒŒæ™¯åè½¬
        aug_image = ColorAugmentation.invert_background(aug_image, prob=0.3)

        # 3. çº¿æ¡é¢œè‰²è°ƒæ•´
        aug_image = ColorAugmentation.adjust_line_color(aug_image, prob=0.4)

        # 4. å¯¹æ¯”åº¦äº®åº¦è°ƒæ•´
        aug_image = ColorAugmentation.adjust_contrast_brightness(aug_image, prob=0.5)

        # 5. éšæœºé¢œè‰²åç§»
        aug_image = ColorAugmentation.random_color_shift(aug_image, prob=0.3)

        return aug_image

class DetectionDataset(Dataset):
    def __init__(self, img_dir: str, csv_dir: str, class_names: list, training=True, img_size=1024):
        """
        Args:
            img_dir: åˆ‡ç‰‡å›¾åƒç›®å½•
            csv_dir: å¯¹åº”CSVç›®å½•
            class_names: ç±»åˆ«åˆ—è¡¨
            training: è®­ç»ƒ/æµ‹è¯•æ¨¡å¼
        """
        self.img_dir = Path(img_dir)
        self.csv_dir = Path(csv_dir)
        self.class_names = class_names
        self.class_to_idx = {name: i for i, name in enumerate(class_names)}
        self.training = training
        self.img_size = img_size
        self.use_color_aug = True
        # æ”¶é›†æ ·æœ¬
        self.samples = []
        for img_path in self.img_dir.glob("*.jpg"):
            csv_path = self.csv_dir / (img_path.stem + ".csv")
            if csv_path.exists():
                self.samples.append((img_path, csv_path))

        # ä½¿ç”¨albumentationsè¿›è¡Œæ•°æ®å¢å¼ºï¼ˆåŒæ­¥å¤„ç†å›¾åƒå’Œè¾¹ç•Œæ¡†ï¼‰
        if training:
            self.transform = self.get_simple_transform()

            #self.transform = self.get_enhance_transform()
        else:
            self.transform = A.Compose([
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], bbox_params=A.BboxParams(
                format='pascal_voc',
                label_fields=['labels']
            ))

        print(f"âœ… åŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")

    def resize_image_and_boxes(self, image, boxes, target_size):
        """
        æ‰‹åŠ¨è°ƒæ•´å›¾åƒå°ºå¯¸å’Œå¯¹åº”çš„è¾¹ç•Œæ¡†

        Args:
            image: åŸå§‹å›¾åƒ (H, W, C)
            boxes: è¾¹ç•Œæ¡†åˆ—è¡¨ [[xmin, ymin, xmax, ymax], ...]
            target_size: ç›®æ ‡å°ºå¯¸

        Returns:
            resized_image: è°ƒæ•´åçš„å›¾åƒ
            resized_boxes: è°ƒæ•´åçš„è¾¹ç•Œæ¡†
        """
        # è·å–åŸå§‹å°ºå¯¸
        orig_height, orig_width = image.shape[:2]

        # è°ƒæ•´å›¾åƒå°ºå¯¸
        resized_image = cv2.resize(image, (target_size, target_size))

        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale_x = target_size / orig_width
        scale_y = target_size / orig_height

        # è°ƒæ•´è¾¹ç•Œæ¡†åæ ‡
        resized_boxes = []
        for box in boxes:
            xmin, ymin, xmax, ymax = box
            # åº”ç”¨ç¼©æ”¾
            new_xmin = xmin * scale_x
            new_ymin = ymin * scale_y
            new_xmax = xmax * scale_x
            new_ymax = ymax * scale_y

            # ç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒèŒƒå›´å†…
            new_xmin = max(0, min(new_xmin, target_size - 1))
            new_ymin = max(0, min(new_ymin, target_size - 1))
            new_xmax = max(0, min(new_xmax, target_size - 1))
            new_ymax = max(0, min(new_ymax, target_size - 1))

            # åªä¿ç•™æœ‰æ•ˆçš„è¾¹ç•Œæ¡†
            if new_xmin < new_xmax and new_ymin < new_ymax:
                resized_boxes.append([new_xmin, new_ymin, new_xmax, new_ymax])

        return resized_image, resized_boxes

    def get_simple_transform(self):
        """é’ˆå¯¹å»ºç­‘å¹³é¢å›¾çš„ç®€åŒ–å¢å¼º"""
        return A.Compose([
                # é¢œè‰²å¢å¼º
                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),

                # å‡ ä½•å˜æ¢ - è¿™äº›ä¼šåŒæ­¥è°ƒæ•´è¾¹ç•Œæ¡†
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),
                A.RandomRotate90(p=0.3),
                A.ShiftScaleRotate(
                    shift_limit=0.05,  # è½»å¾®å¹³ç§»
                    scale_limit=0.1,  # è½»å¾®ç¼©æ”¾
                    rotate_limit=5,  # å°è§’åº¦æ—‹è½¬
                    p=0.5
                ),
                # ä¸“é—¨çš„é¢œè‰²å¢å¼º
                A.Lambda(name='ColorAugmentation',
                         image=lambda image, **kwargs: self.apply_color_augmentation(image),
                         p=0.8),  # 80%æ¦‚ç‡åº”ç”¨é¢œè‰²å¢å¼º
                # é’ˆå¯¹å»ºç­‘å¹³é¢å›¾çš„å¢å¼º
                A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),
                A.Blur(blur_limit=3, p=0.2),

                # è°ƒæ•´å°ºå¯¸åˆ°img_size
                A.Resize(self.img_size, self.img_size, always_apply=True),

                # æ ‡å‡†åŒ–å¹¶è½¬æ¢ä¸ºTensor
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], bbox_params=A.BboxParams(
                format='pascal_voc',  # ä½¿ç”¨[x_min, y_min, x_max, y_max]æ ¼å¼
                label_fields=['labels']
            ))

    def get_enhance_transform(self):
        return A.Compose([
                # åŸºç¡€å‡ ä½•å˜æ¢
                A.OneOf([
                    A.ShiftScaleRotate(
                        shift_limit=0.1,  # å¢åŠ å¹³ç§»å¹…åº¦
                        scale_limit=0.2,  # å¢åŠ ç¼©æ”¾èŒƒå›´
                        rotate_limit=15,  # å¢åŠ æ—‹è½¬è§’åº¦
                        border_mode=cv2.BORDER_CONSTANT,
                        value=0,  # ç”¨é»‘è‰²å¡«å……
                        p=0.7
                    ),
                    A.Affine(
                        scale=(0.8, 1.2),
                        translate_percent=(-0.1, 0.1),
                        rotate=(-15, 15),
                        shear=(-5, 5),  # æ·»åŠ å‰ªåˆ‡å˜æ¢
                        p=0.3
                    )
                ], p=0.8),

                # é•œåƒç¿»è½¬
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.3),

                # é€è§†å˜æ¢ - æ¨¡æ‹Ÿä¸åŒè§†è§’
                A.Perspective(
                    scale=(0.05, 0.1),  # è½»å¾®é€è§†
                    keep_size=True,
                    p=0.3
                ),

                # é¢œè‰²å¢å¼º - æ›´ä¸°å¯Œçš„é¢œè‰²å˜åŒ–
                A.OneOf([
                    A.ColorJitter(
                        brightness=0.3,  # å¢åŠ äº®åº¦å˜åŒ–
                        contrast=0.3,
                        saturation=0.3,
                        hue=0.1,
                        p=1.0
                    ),
                    A.HueSaturationValue(
                        hue_shift_limit=10,
                        sat_shift_limit=20,
                        val_shift_limit=10,
                        p=1.0
                    ),
                    A.RGBShift(
                        r_shift_limit=20,
                        g_shift_limit=20,
                        b_shift_limit=20,
                        p=1.0
                    )
                ], p=0.7),

                # å™ªå£°å’Œæ¨¡ç³Š - æ¨¡æ‹Ÿä¸åŒå›¾åƒè´¨é‡
                A.OneOf([
                    A.GaussNoise(var_limit=(10.0, 100.0), p=0.5),
                    A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.3),
                    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),
                ], p=0.4),

                A.OneOf([
                    A.Blur(blur_limit=3, p=0.5),
                    A.MotionBlur(blur_limit=5, p=0.3),
                    A.MedianBlur(blur_limit=3, p=0.2),
                ], p=0.3),

                # é’ˆå¯¹å»ºç­‘å›¾çº¸çš„ç‰¹æ®Šå¢å¼º
                A.OneOf([
                    # æ¨¡æ‹Ÿå›¾çº¸è´¨é‡å˜åŒ–
                    A.RandomGamma(gamma_limit=(80, 120), p=0.5),
                    A.RandomBrightnessContrast(
                        brightness_limit=0.2,
                        contrast_limit=0.2,
                        p=0.5
                    ),
                ], p=0.4),

                # æ¨¡æ‹Ÿé®æŒ¡å’Œç¼ºå¤±
                A.OneOf([
                    A.CoarseDropout(
                        max_holes=8,
                        max_height=32,
                        max_width=32,
                        min_holes=1,
                        min_height=8,
                        min_width=8,
                        fill_value=0,
                        p=0.5
                    ),
                    A.GridDropout(
                        unit_size_min=16,
                        unit_size_max=64,
                        holes_number_x=4,
                        holes_number_y=4,
                        shift_x=0,
                        shift_y=0,
                        random_offset=True,
                        fill_value=0,
                        p=0.3
                    )
                ], p=0.3),

                # å›¾åƒè´¨é‡é€€åŒ–
                A.ImageCompression(quality_lower=60, quality_upper=95, p=0.2),

                # æ ‡å‡†åŒ–
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], bbox_params=A.BboxParams(
                format='pascal_voc',
                label_fields=['labels'],
                min_area=1,  # ç¡®ä¿å°ç›®æ ‡ä¸è¢«è¿‡æ»¤
                min_visibility=0.1  # å…è®¸éƒ¨åˆ†å¯è§çš„ç›®æ ‡
            ))

    def apply_color_augmentation(self, image, **kwargs):
        """åº”ç”¨è‡ªå®šä¹‰é¢œè‰²å¢å¼º"""
        if not self.training or not self.use_color_aug:
            return image

        # éšæœºé€‰æ‹©é¢œè‰²å¢å¼ºç­–ç•¥
        strategies = [
            lambda img: ColorAugmentation.apply_all_color_augmentations(img),
            lambda img: ColorAugmentation.change_background_color(img, prob=0.5),
            lambda img: ColorAugmentation.invert_background(img, prob=0.5),
            lambda img: ColorAugmentation.adjust_line_color(img, prob=0.6),
            lambda img: ColorAugmentation.apply_blueprint(img),
            lambda img: img  # ä¿æŒåŸå›¾
        ]

        strategy = random.choice(strategies)
        augmented_image = strategy(image)

        return augmented_image
    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, csv_path = self.samples[idx]

        # ä½¿ç”¨OpenCVåŠ è½½å›¾åƒï¼ˆalbumentationséœ€è¦numpyæ•°ç»„ï¼‰
        image = cv2.imread(str(img_path))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        orig_size = image.shape[:2]  # (H, W)

        # åŠ è½½æ ‡æ³¨
        df = pd.read_csv(csv_path)
        boxes = []
        labels = []

        for _, row in df.iterrows():
            # ä½¿ç”¨ç»å¯¹åæ ‡ï¼ˆä¸æ˜¯å½’ä¸€åŒ–çš„ï¼‰
            # albumentationsä¼šåœ¨å˜æ¢åè‡ªåŠ¨è°ƒæ•´è¾¹ç•Œæ¡†
            xmin = row['xmin']
            ymin = row['ymin']
            xmax = row['xmax']
            ymax = row['ymax']

            # ç¡®ä¿è¾¹ç•Œæ¡†æœ‰æ•ˆ
            if xmin < xmax and ymin < ymax:
                boxes.append([xmin, ymin, xmax, ymax])
                labels.append(self.class_to_idx[convert_class(row['class'])])

        if len(boxes) == 0:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆè¾¹ç•Œæ¡†ï¼Œåˆ›å»ºç©ºç›®æ ‡
            boxes = [[0, 0, 1, 1]]  # è™šæ‹Ÿå°æ¡†
            labels = [0]  # é»˜è®¤ç±»åˆ«

        # åº”ç”¨æ•°æ®å¢å¼ºï¼ˆåŒæ­¥å¤„ç†å›¾åƒå’Œè¾¹ç•Œæ¡†ï¼‰
        if self.transform is not None:
            transformed = self.transform(
                image=image,
                bboxes=boxes,
                labels=labels
            )
            image = transformed['image']  # å·²ç»æ˜¯Tensor [3, H, W]
            boxes = transformed['bboxes']
            labels = transformed['labels']

        # è½¬æ¢è¾¹ç•Œæ¡†ä¸ºTensorå¹¶å½’ä¸€åŒ–
        if len(boxes) > 0:
            boxes = torch.tensor(boxes, dtype=torch.float32)
            # å½’ä¸€åŒ–åˆ° [0, 1]
            boxes[:, 0] = boxes[:, 0] / self.img_size  # xmin
            boxes[:, 1] = boxes[:, 1] / self.img_size  # ymin
            boxes[:, 2] = boxes[:, 2] / self.img_size  # xmax
            boxes[:, 3] = boxes[:, 3] / self.img_size  # ymax
        else:
            boxes = torch.zeros((0, 4), dtype=torch.float32)

        labels = torch.tensor(labels, dtype=torch.int64)

        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": torch.tensor([idx]),
            "orig_size": torch.tensor(orig_size)
        }

        return image, target

    def tensor_to_numpy(self, tensor_image):
        """
        å°†æ ‡å‡†åŒ–åçš„tensorå›¾åƒè½¬æ¢å›numpyæ ¼å¼ç”¨äºä¿å­˜
        """
        # åæ ‡å‡†åŒ–
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        image = tensor_image * std + mean

        # è½¬æ¢ä¸ºnumpyå¹¶è°ƒæ•´èŒƒå›´åˆ°0-255
        image = image.mul(255).byte()
        image = image.numpy().transpose(1, 2, 0)  # CHW -> HWC

        return image

    def get_absolute_boxes(self, normalized_boxes):
        """
        å°†å½’ä¸€åŒ–çš„è¾¹ç•Œæ¡†åæ ‡è½¬æ¢å›ç»å¯¹åæ ‡
        """
        if len(normalized_boxes) == 0:
            return []

        boxes_abs = normalized_boxes.clone()
        boxes_abs[:, 0] = boxes_abs[:, 0] * self.img_size  # xmin
        boxes_abs[:, 1] = boxes_abs[:, 1] * self.img_size  # ymin
        boxes_abs[:, 2] = boxes_abs[:, 2] * self.img_size  # xmax
        boxes_abs[:, 3] = boxes_abs[:, 3] * self.img_size  # ymax

        return boxes_abs.numpy().astype(int)

    def save_boxes_to_csv(self, boxes, labels, csv_path, img_filename):
        """
        å°†è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ä¿å­˜ä¸ºCSVæ–‡ä»¶
        """
        data = []
        for i, (box, label_idx) in enumerate(zip(boxes, labels)):
            class_name = self.class_names[label_idx] if label_idx < len(self.class_names) else "unknown"
            data.append({
                'filename': img_filename,
                'xmin': box[0],
                'ymin': box[1],
                'xmax': box[2],
                'ymax': box[3],
                'class': class_name
            })

        df = pd.DataFrame(data)
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¾¹ç•Œæ¡†ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameä½†ä¿æŒåˆ—ç»“æ„
        if df.empty:
            df = pd.DataFrame(columns=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'class'])
        df.to_csv(csv_path, index=False)

    def save_augmented_samples(self,save_augmented_dir,  num_samples=50, start_idx=0):
        """
        ä¿å­˜ç»è¿‡æ•°æ®å¢å¼ºåçš„æ ·æœ¬åˆ°æŒ‡å®šæ–‡ä»¶å¤¹

        Args:
            num_samples: è¦ä¿å­˜çš„æ ·æœ¬æ•°é‡
            start_idx: èµ·å§‹ç´¢å¼•
        """
        if not save_augmented_dir:
            print("âŒ æœªè®¾ç½®ä¿å­˜ç›®å½•ï¼Œè¯·åˆå§‹åŒ–æ—¶æŒ‡å®š save_augmented_dir")
            return

        saved_count = 0
        idx = start_idx

        while saved_count < num_samples and idx < len(self.samples):
            try:
                # è·å–å¢å¼ºåçš„æ ·æœ¬
                image_tensor, target = self.__getitem__(idx)

                # è½¬æ¢å›¾åƒä¸ºå¯ä¿å­˜æ ¼å¼
                image_np = self.tensor_to_numpy(image_tensor)

                # è·å–è¾¹ç•Œæ¡†çš„ç»å¯¹åæ ‡
                boxes_abs = self.get_absolute_boxes(target["boxes"])
                labels = target["labels"].numpy()

                # ç”Ÿæˆæ–‡ä»¶å
                orig_name = self.samples[idx][0].stem
                img_filename = f"aug_{orig_name}_{saved_count:04d}.jpg"
                csv_filename = f"aug_{orig_name}_{saved_count:04d}.csv"
                img_path = os.path.join(save_augmented_dir, f"{img_filename}")
                csv_path = os.path.join(save_augmented_dir, f"{csv_filename}")

                # ä¿å­˜å›¾åƒ
                cv2.imwrite(str(img_path), cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))

                # ä¿å­˜CSVæ ‡æ³¨
                self.save_boxes_to_csv(boxes_abs, labels, csv_path, img_filename)

                saved_count += 1
                print(f"âœ… ä¿å­˜å¢å¼ºæ ·æœ¬ {saved_count}/{num_samples}: {img_filename}")

            except Exception as e:
                print(f"âŒ ä¿å­˜æ ·æœ¬ {idx} å¤±è´¥: {e}")

            idx += 1

        print(f"ğŸ‰ æˆåŠŸä¿å­˜ {saved_count} ä¸ªå¢å¼ºæ ·æœ¬åˆ° {save_augmented_dir}")

if __name__ == "__main__":
    class_names = ["wall", "door", "window", "column"]
    dataset = DetectionDataset(
        img_dir="cvt_images",
        csv_dir="cvt_images",
        class_names=class_names,
        training=True
    )
    save_augmented_dir = "vis_images"
    dataset.save_augmented_samples(save_augmented_dir=save_augmented_dir, num_samples=100)