# detector.py
import os

import torch
import torchvision.transforms as T
from PIL import Image
import cv2
from typing import List, Dict, Tuple
import json

from data.wall_classifier import WallClassifier, get_transforms
from models.default_model import DummyModel
from utils.shape import Shape, ImageAnnotation, save_image_annotations_to_jsonl
from validate import decode_outputs


class IntegratedDetector:
    """é›†æˆäº†ç›®æ ‡æ£€æµ‹å’Œå¢™åˆ†ç±»çš„æ£€æµ‹å™¨"""

    def __init__(self,
                 detection_model_path: str = "training_output/best_model.pth",
                 classification_model_path: str = "best_wall_classifier.pth",
                 device: str = None):

        self.device = torch.device(device if device else
                                   ("cuda" if torch.cuda.is_available() else "cpu"))

        # åŠ è½½ç›®æ ‡æ£€æµ‹æ¨¡å‹
        self.detection_model = DummyModel(num_classes=4).to(self.device)
        self.detection_model.load_state_dict(torch.load(detection_model_path, map_location=self.device))
        self.detection_model.eval()

        # åŠ è½½å¢™åˆ†ç±»æ¨¡å‹
        self.classification_model = WallClassifier(num_classes=23, backbone='efficientnet_b0')
        self.classification_model.load_state_dict(torch.load(classification_model_path, map_location=self.device))
        self.classification_model.to(self.device)
        self.classification_model.eval()

        # ç±»åˆ«åç§°
        self.detection_class_names = ["wall", "door", "window", "column"]
        self.wall_class_names = [f"WALL{i}" for i in range(1, 24)]

        # å›¾åƒå˜æ¢
        self.detection_transform = T.Compose([
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        self.classification_transform, _ = get_transforms(224)

        print(f"âœ… é›†æˆæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç›®æ ‡æ£€æµ‹æ¨¡å‹: {detection_model_path}")
        print(f"   å¢™åˆ†ç±»æ¨¡å‹: {classification_model_path}")
        print(f"   è®¾å¤‡: {self.device}")


    def slice_image(self, image_path: str, slice_size: int = 1024, overlap: float = 0.1) -> List[Dict]:
        """å°†å¤§å›¾åˆ‡ç‰‡æˆå°å›¾ç”¨äºæ£€æµ‹"""
        image = Image.open(image_path).convert('RGB')
        img_w, img_h = image.size

        step = int(slice_size * (1 - overlap))
        slices = []
        slice_idx = 0

        y = 0
        while y < img_h:
            x = 0
            while x < img_w:
                # è®¡ç®—è£å‰ªåŒºåŸŸ
                x2 = x + slice_size
                y2 = y + slice_size
                crop_x2 = min(x2, img_w)
                crop_y2 = min(y2, img_h)

                # è£å‰ªå›¾åƒ
                patch = image.crop((x, y, crop_x2, crop_y2))

                # å¦‚æœå°ºå¯¸ä¸è¶³ï¼Œè¿›è¡Œpadding
                pad_right = slice_size - patch.width
                pad_bottom = slice_size - patch.height

                if pad_right > 0 or pad_bottom > 0:
                    padded_patch = Image.new('RGB', (slice_size, slice_size), (254, 254, 254))
                    padded_patch.paste(patch, (0, 0))
                    patch = padded_patch

                # ä¿å­˜åˆ‡ç‰‡ä¿¡æ¯
                slices.append({
                    'slice_idx': slice_idx,
                    'patch': patch,
                    'original_coords': (x, y, crop_x2, crop_y2),
                    'slice_coords': (0, 0, slice_size, slice_size),
                    'padding': (pad_right, pad_bottom)
                })

                slice_idx += 1
                x += step
            y += step

        print(f"âœ… å›¾åƒåˆ‡ç‰‡å®Œæˆ: {len(slices)} ä¸ªåˆ‡ç‰‡")
        return slices

    def detect_in_slice(self, patch, confidence_threshold=0.3) -> List[Dict]:
        """åœ¨å•ä¸ªåˆ‡ç‰‡ä¸­è¿›è¡Œç›®æ ‡æ£€æµ‹"""
        # é¢„å¤„ç†
        input_tensor = self.detection_transform(patch).unsqueeze(0).to(self.device)

        # æ¨ç†
        with torch.no_grad():
            outputs = self.detection_model(input_tensor)

        # è§£ç è¾“å‡º
        detections = decode_outputs(outputs, confidence_threshold)
        batch_detections = detections[0]  # å–ç¬¬ä¸€ä¸ªbatch

        results = []
        for detection in batch_detections:
            xmin, ymin, xmax, ymax, confidence, class_id = detection
            class_name = self.detection_class_names[class_id]

            results.append({
                'xmin': int(xmin),
                'ymin': int(ymin),
                'xmax': int(xmax),
                'ymax': int(ymax),
                'class': class_name,
                'confidence': confidence,
                'class_id': class_id
            })

        return results

    def classify_wall(self, image_patch) -> Dict:
        """å¯¹å¢™å›¾åƒå—è¿›è¡Œåˆ†ç±»"""
        # é¢„å¤„ç†
        input_tensor = self.classification_transform(image_patch).unsqueeze(0).to(self.device)

        # æ¨ç†
        with torch.no_grad():
            outputs = self.classification_model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            pred_prob, pred_class = torch.max(probabilities, 1)

        # è·å–åŸå§‹åˆ†ç±»ç»“æœ
        original_class_idx = pred_class.item()
        class_name = self.wall_class_names[original_class_idx]
        # é‡æ˜ å°„ç‰¹å®šç±»åˆ«åˆ° WALL1
        if class_name in ["WALL13", "WALL20", "WALL23"]:
            class_name = "WALL1"
            original_class_idx = 0  # WALL1 çš„ç´¢å¼•
        return {
            'class_name': class_name,
            'class_idx': original_class_idx,
            'confidence': pred_prob.item()
        }

    def merge_detections(self, slices_detections: List[Dict], original_size: Tuple) -> List[Dict]:
        """åˆå¹¶æ‰€æœ‰åˆ‡ç‰‡çš„æ£€æµ‹ç»“æœåˆ°åŸå›¾åæ ‡"""
        original_w, original_h = original_size
        all_detections = []

        for slice_info, detections in slices_detections:
            slice_x, slice_y, crop_x2, crop_y2 = slice_info['original_coords']

            for detection in detections:
                # è½¬æ¢åæ ‡åˆ°åŸå›¾
                xmin_orig = detection['xmin'] + slice_x
                ymin_orig = detection['ymin'] + slice_y
                xmax_orig = detection['xmax'] + slice_x
                ymax_orig = detection['ymax'] + slice_y

                # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                xmin_orig = max(0, min(xmin_orig, original_w))
                ymin_orig = max(0, min(ymin_orig, original_h))
                xmax_orig = max(0, min(xmax_orig, original_w))
                ymax_orig = max(0, min(ymax_orig, original_h))

                # åªä¿ç•™æœ‰æ•ˆçš„æ£€æµ‹
                if xmax_orig > xmin_orig and ymax_orig > ymin_orig:
                    detection_orig = detection.copy()
                    detection_orig.update({
                        'xmin': int(xmin_orig),
                        'ymin': int(ymin_orig),
                        'xmax': int(xmax_orig),
                        'ymax': int(ymax_orig)
                    })
                    all_detections.append(detection_orig)

        # ç®€å•çš„éæå¤§å€¼æŠ‘åˆ¶å»é‡
        filtered_detections = self.category_aware_nms(all_detections, iou_threshold=0.4)
        #filtered_detections = all_detections
        return filtered_detections

    def category_aware_nms(self, detections: List[Dict], iou_threshold: float = 0.5) -> List[Dict]:
        """æŒ‰ç±»åˆ«æ„ŸçŸ¥çš„éæå¤§å€¼æŠ‘åˆ¶ï¼Œå¯¹columnç±»åˆ«è¿›è¡Œåˆå¹¶"""
        if not detections:
            return []

        # æŒ‰ç±»åˆ«åˆ†ç»„
        detections_by_class = {}
        for det in detections:
            class_id = det['class_id']
            if class_id not in detections_by_class:
                detections_by_class[class_id] = []
            detections_by_class[class_id].append(det)

        filtered_detections = []

        for class_id, class_detections in detections_by_class.items():
            class_name = self.detection_class_names[class_id]

            if class_name == "column":
                # å¯¹columnç±»åˆ«è¿›è¡Œåˆå¹¶
                merged_detections = self.merge_columns(class_detections, iou_threshold)
                filtered_detections.extend(merged_detections)
            else:
                # å¯¹å…¶ä»–ç±»åˆ«è¿›è¡Œæ ‡å‡†NMS
                nms_detections = self.nms_for_class(class_detections, iou_threshold)
                filtered_detections.extend(nms_detections)

        return filtered_detections

    def nms_for_class(self, detections: List[Dict], iou_threshold: float) -> List[Dict]:
        """å¯¹å•ä¸ªç±»åˆ«è¿›è¡Œæ ‡å‡†éæå¤§å€¼æŠ‘åˆ¶"""
        if not detections:
            return []

        # æŒ‰ç½®ä¿¡åº¦æ’åº
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)

        keep = []
        while detections:
            # å–ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹
            best = detections.pop(0)
            keep.append(best)

            # ç§»é™¤ä¸å½“å‰æ£€æµ‹é‡å åº¦é«˜çš„æ£€æµ‹
            detections = [det for det in detections
                          if self.calculate_iou(best, det) < iou_threshold]

        return keep

    def merge_columns(self, detections: List[Dict], iou_threshold: float) -> List[Dict]:
        """åˆå¹¶é‡å çš„columnæ£€æµ‹æ¡†"""
        if not detections:
            return []

        # æŒ‰ç½®ä¿¡åº¦æ’åº
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)

        merged_groups = []

        while detections:
            # å–å½“å‰æœ€é«˜ç½®ä¿¡åº¦çš„æ£€æµ‹ä½œä¸ºç§å­
            seed = detections.pop(0)
            current_group = [seed]

            # å¯»æ‰¾ä¸ç§å­é‡å çš„æ‰€æœ‰æ£€æµ‹
            remaining = []
            for det in detections:
                iou = self.calculate_iou(seed, det)
                if iou > iou_threshold:
                    current_group.append(det)
                else:
                    remaining.append(det)

            # åˆå¹¶å½“å‰ç»„ä¸­çš„æ‰€æœ‰æ£€æµ‹æ¡†
            if len(current_group) > 1:
                # è®¡ç®—åˆå¹¶åçš„åŒ…å›´æ¡†ï¼ˆå–æ‰€æœ‰æ¡†çš„å¹¶é›†ï¼‰
                xmin = min(det['xmin'] for det in current_group)
                ymin = min(det['ymin'] for det in current_group)
                xmax = max(det['xmax'] for det in current_group)
                ymax = max(det['ymax'] for det in current_group)

                # å–ç»„å†…æœ€é«˜ç½®ä¿¡åº¦
                confidence = max(det['confidence'] for det in current_group)

                # åˆ›å»ºåˆå¹¶åçš„æ£€æµ‹ç»“æœ
                merged_detection = {
                    'xmin': xmin,
                    'ymin': ymin,
                    'xmax': xmax,
                    'ymax': ymax,
                    'class': 'column',
                    'confidence': confidence,
                    'class_id': current_group[0]['class_id']  # ä¿æŒç›¸åŒçš„class_id
                }
                merged_groups.append(merged_detection)
            else:
                # å¦‚æœæ²¡æœ‰é‡å çš„ï¼Œç›´æ¥ä¿ç•™åŸæ£€æµ‹
                merged_groups.append(seed)

            detections = remaining

        return merged_groups

    def calculate_iou(self, box1: Dict, box2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªåŒ…å›´æ¡†çš„äº¤å¹¶æ¯”"""
        # è®¡ç®—äº¤é›†åŒºåŸŸ
        x1 = max(box1['xmin'], box2['xmin'])
        y1 = max(box1['ymin'], box2['ymin'])
        x2 = min(box1['xmax'], box2['xmax'])
        y2 = min(box1['ymax'], box2['ymax'])

        # è®¡ç®—äº¤é›†é¢ç§¯
        intersection = max(0, x2 - x1) * max(0, y2 - y1)

        # è®¡ç®—å„è‡ªé¢ç§¯
        area1 = (box1['xmax'] - box1['xmin']) * (box1['ymax'] - box1['ymin'])
        area2 = (box2['xmax'] - box2['xmin']) * (box2['ymax'] - box2['ymin'])

        # è®¡ç®—å¹¶é›†é¢ç§¯
        union = area1 + area2 - intersection

        # é¿å…é™¤ä»¥é›¶
        if union == 0:
            return 0.0

        return intersection / union

    def detect(self, image_path: str, confidence_threshold: float = 0.3) -> List[Dict]:
        """å®Œæ•´çš„æ£€æµ‹æµç¨‹"""
        print(f"ğŸ” å¼€å§‹å¤„ç†å›¾åƒ: {image_path}")

        # 1. åŠ è½½åŸå›¾å¹¶è·å–å°ºå¯¸
        original_image = Image.open(image_path).convert('RGB')
        original_w, original_h = original_image.size
        print(f"   åŸå›¾å°ºå¯¸: {original_w} x {original_h}")

        # 2. åˆ‡ç‰‡
        slices = self.slice_image(image_path, slice_size=1024, overlap=0.1)

        # 3. å¯¹æ¯ä¸ªåˆ‡ç‰‡è¿›è¡Œç›®æ ‡æ£€æµ‹
        slices_detections = []
        for slice_info in slices:
            patch = slice_info['patch']
            slice_detections = self.detect_in_slice(patch, confidence_threshold)
            slices_detections.append((slice_info, slice_detections))


        # 4. åˆå¹¶æ£€æµ‹ç»“æœåˆ°åŸå›¾åæ ‡
        merged_detections = self.merge_detections(slices_detections, (original_w, original_h))
        print(f"   åˆå¹¶åæ£€æµ‹åˆ° {len(merged_detections)} ä¸ªç›®æ ‡")

        # 5. å¯¹å¢™ç›®æ ‡è¿›è¡Œåˆ†ç±»
        final_detections = []
        wall_count = 0

        for detection in merged_detections:
            if detection['class'] == 'wall':
                # ä»åŸå›¾ä¸­è£å‰ªå¢™åŒºåŸŸ
                xmin, ymin, xmax, ymax = detection['xmin'], detection['ymin'], detection['xmax'], detection['ymax']

                # ç¡®ä¿åæ ‡æœ‰æ•ˆ
                if xmax > xmin and ymax > ymin:
                    wall_patch = original_image.crop((xmin, ymin, xmax, ymax))

                    # åˆ†ç±»å¢™ç±»å‹
                    classification_result = self.classify_wall(wall_patch)

                    # æ›´æ–°æ£€æµ‹ç»“æœ
                    detection['class'] = classification_result['class_name'].lower()
                    #detection['class'] = 'wall1'
                    detection['wall_confidence'] = classification_result['confidence']
                    wall_count += 1

            final_detections.append(detection)

        print(f"   å¯¹ {wall_count} ä¸ªå¢™ç›®æ ‡è¿›è¡Œäº†åˆ†ç±»")
        print(f"âœ… å¤„ç†å®Œæˆ: æ€»å…± {len(final_detections)} ä¸ªæ£€æµ‹ç›®æ ‡")

        return final_detections


# ä¸ºäº†å…¼å®¹ä½ çš„ç°æœ‰ä»£ç ï¼Œä¿ç•™DummyDetectoråç§°
class DummyDetector(IntegratedDetector):
    """ä¸ºäº†å…¼å®¹æ€§ï¼Œå°†IntegratedDetectoré‡å‘½åä¸ºDummyDetector"""

    def __init__(self,
                 detection_model_path: str = "training_output/best_model.pth",
                 classification_model_path: str = "best_wall_classifier.pth"):
        super().__init__(detection_model_path, classification_model_path)

    def detect(self, image_path: str) -> List:
        """
        æ£€æµ‹æ¥å£ï¼Œè¿”å›Shapeå¯¹è±¡åˆ—è¡¨
        ä¸ºäº†å…¼å®¹ä½ çš„æœåŠ¡æ¡†æ¶
        """
        detections = super().detect(image_path, confidence_threshold=0.5)

        # è½¬æ¢ä¸ºShapeå¯¹è±¡ - æ ¹æ®Shapeç±»å®šä¹‰ä¿®æ­£æ ¼å¼
        shapes = []
        for detection in detections:
            shape = Shape(
                label=detection['class'],
                points=[
                    [float(detection['xmin']), float(detection['ymin'])],  # [[x_min, y_min]
                    [float(detection['xmax']), float(detection['ymax'])]  # [x_max, y_max]]
                ],
                confidence_score=float(detection.get('wall_confidence', detection['confidence'])),
                shape_type="rectangle",
                group_id=None,
                flags={}
            )
            shapes.append(shape)

        return shapes

if __name__ == '__main__':
    from pathlib import Path
    detector = DummyDetector()

    # 1. åœ¨è¿™é‡ŒåŠ è½½æ‚¨çš„æ¨¡å‹
    print("æ¨¡å‹åŠ è½½ä¸­...")
    # 2. è¯»å–è¾“å…¥æ•°æ®
    # å‡è®¾è¾“å…¥æ˜¯ä¸€ä¸ªåä¸º data.csv çš„æ–‡ä»¶
    input_file = "F:\\é¡¹ç›®æ•°æ®\\ä¸­çŸ³åŒ–æ¯”èµ›\\ä»¥å›¾æ‰¾å›¾å·¥ç¨‹å›¾çº¸æ™ºèƒ½è¯†åˆ«\\åˆèµ›å‘å¸ƒ\\éªŒè¯é›†å›¾ç‰‡"
    output_path = "output"
    print(f"æ­£åœ¨è¯»å–è¾“å…¥æ–‡ä»¶: {input_file}")
    # è·å–å¾…å¤„ç†å›¾ç‰‡
    drawing_files = []
    img_dir = Path(input_file)
    for img_path in img_dir.glob("*.jpg"):
        drawing_files.append(img_path)

    all_annotations = []
    for drawing_file in drawing_files:
        drawing_path = drawing_file
        img = cv2.imread(str(drawing_path))  # ç¡®ä¿ä½¿ç”¨å­—ç¬¦ä¸²è·¯å¾„
        h, w = (img.shape[0], img.shape[1]) if img is not None else (3118, 4414)
        # 3. è¿›è¡Œæ¨¡å‹é¢„æµ‹
        # è°ƒç”¨æ£€æµ‹å™¨
        print(f"æ­£åœ¨å¤„ç†å›¾åƒ: {drawing_file.name}")
        shapes = detector.detect(str(drawing_path))  # ç¡®ä¿ä½¿ç”¨å­—ç¬¦ä¸²è·¯å¾„
        print(f"æ¨¡å‹é¢„æµ‹å®Œæˆï¼æ£€æµ‹åˆ° {len(shapes)} ä¸ªç›®æ ‡")

        # è¿˜éœ€è¦ä»shapesä¸­è¿‡æ»¤æ‰å›¾ä¾‹ä¸­æ²¡æœ‰çš„ç±»åˆ«

        # åˆ›å»º ImageAnnotation å¯¹è±¡
        annotation = ImageAnnotation(
            image_path=str(drawing_file.name),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            image_height=h,
            image_width=w,
            shapes=shapes
        )
        all_annotations.append(annotation)

    # ä¿å­˜ä¸º .jsonl
    output_file = os.path.join(output_path, 'results.jsonl')
    save_image_annotations_to_jsonl(all_annotations, output_file)